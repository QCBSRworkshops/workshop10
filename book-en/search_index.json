[["index.html", "Workshop 10: Advanced Multivariate Analyses in R QCBS R Workshop Series Preface 0.1 Code of conduct 0.2 Contributors 0.3 Contributing", " Workshop 10: Advanced Multivariate Analyses in R QCBS R Workshop Series Developed and maintained by the contributors of the QCBS R Workshop Series1 2021-03-24 21:38:59 Preface The QCBS R Workshop Series is a series of 10 workshops that walks participants through the steps required to use R for a wide array of statistical analyses relevant to research in biology and ecology. These open-access workshops were created by members of the QCBS both for members of the QCBS and the larger community. The content of this workshop has been peer-reviewed by several QCBS members. If you would like to suggest modifications, please contact the current series coordinators, listed on the main Github page. 0.1 Code of conduct The QCBS R Workshop Series and the QCBS R Symposium are venues dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. Participants, presenters and organizers of the workshop series and other related activities accept this Code of Conduct when being present at any workshop-related activities. We do not tolerate behaviour that is disrespectful or that excludes, intimidates, or causes discomfort to others. We do not tolerate discrimination or harassment based on characteristics that include, but are not limited to, gender identity and expression, sexual orientation, disability, physical appearance, body size, citizenship, nationality, ethnic or social origin, pregnancy, familial status, genetic information, religion or belief (or lack thereof), membership of a national minority, property, age, education, socio-economic status, technical choices, and experience level. It applies to all spaces managed by or affiliated with the workshop, including, but not limited to, workshops, email lists, and online forums such as GitHub, Slack and Twitter. 0.1.1 Expected behaviour All participants are expected to show respect and courtesy to others. All interactions should be professional regardless of platform: either online or in-person. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all workshop events and platforms: Use welcoming and inclusive language Be respectful of different viewpoints and experiences Gracefully accept constructive criticism Focus on what is best for the community Show courtesy and respect towards other community members 0.1.2 Unacceptable behaviour Examples of unacceptable behaviour by participants at any workshop event/platform include: written or verbal comments which have the effect of excluding people on the - basis of membership of any specific group; causing someone to fear for their safety, such as through stalking or intimidation; violent threats or language directed against another person; the display of sexual or violent images; unwelcome sexual attention; nonconsensual or unwelcome physical contact; insults or put-downs; sexist, racist, homophobic, transphobic, ableist, or exclusionary jokes; incitement to violence, suicide, or self-harm; continuing to initiate interaction (including photography or recording) with - someone after being asked to stop; publication of private communication without consent. 0.2 Contributors Originally developed by: Contributed with changes to the presentation: Contributed with changes to the written material: Contributed by reporting issues and suggesting modifications: 0.3 Contributing Under construction. The QCBS R Workshop Series is part of the Québec Centre for Biodiversity Science, and is maintained by the series coordinators and graduent student, postdoctoral, and research professional members. The contributors for this workshop can be accessed here.↩︎ "],["learning-objectives.html", "Chapter 1 Learning objectives", " Chapter 1 Learning objectives In this workshop, you will learn how to perform advanced multivariate analyses on community data. This workshop concentrates on constrained methods such as redundancy analysis (RDA), multivariate regression tree (MRT) and linear discriminant analysis (LDA) to explore how environmental variables may be driving patterns in species assemblage across sites. "],["preparing-for-the-workshop.html", "Chapter 2 Preparing for the workshop", " Chapter 2 Preparing for the workshop All workshop materials are found at github.com/QCBSRworkshops/workshop10. This includes an R script which contains all code chunks shown in this book. For this workshop, we will be working with the following datasets: DoubsEnv DoubsSpe DoubsSpa Test data for linear discriminant analyses To download this data, do right click + save on the page that opens. You should also make sure you have downloaded, installed, and loaded these packages: vegan (for multivariate analyses) labdsv (for identification of significant indicator species in the multivariate regression tree analysis) MASS (for linear discriminant analysis) mvpart* (for multivariate regression trees) ggplot2 (for plotting some results) # Install the required packages install.packages(&quot;vegan&quot;) install.packages(&quot;labdsv&quot;) install.packages(&quot;MASS&quot;) install.packages(&quot;ggplot2&quot;) # install mvpart from package archive file install.packages(&quot;remotes&quot;) remotes::install_url(&quot;https://cran.r-project.org/src/contrib/Archive/mvpart/mvpart_1.6-2.tar.gz&quot;) *The mvpart package is no longer hosted on CRAN, but is still available from the archives. To install mvpart, you can also download the .tar.gz version from here and go to the “Packages” tab on the bottom right panel of R Studio, and click on “Install Packages”. Choose to install from Package Archive file, and select the .tar.gz for mvpart. # Load the required packages library(vegan) library(labdsv) library(MASS) library(mvpart) library(ggplot2) "],["why-advanced-multivariate-methods.html", "Chapter 3 Why “advanced multivariate methods”?", " Chapter 3 Why “advanced multivariate methods”? The previous workshop presented the basics of multivariate analyses: How to choose appropriate distance metrics and transformations Hierarchical clustering Unconstrained ordinations Principal component analysis Principal coordinate Analysis Correspondence analysis Nonmetric multidimensional scaling The present workshop builds on this knowledge, and will focus on constrained analyses. All the methods overviewed during Workshop 9 allowed us to find patterns in the community composition data or in the descriptors, but not to explore how environmental variables could be driving these patterns. With constrained analyses, such as redundancy analysis (RDA), linear discriminant analysis (LDA) and multivariate regression tree (MRT), one can describe and predict relationships between community composition data and environmental variables. "],["exploration.html", "Chapter 4 Exploring the Doubs River dataset 4.1 Exploring the fish community dataset 4.2 Exploring the environmental dataset", " Chapter 4 Exploring the Doubs River dataset We will be using the Doubs River dataset (Verneaux 1973) for this workshop. DoubsSpe.csv is a data frame of fish community data where the first column contains site names from 1 to 30 and the remaining columns are fish taxa (27 species). The taxa columns are populated by fish abundance data (counts). DoubsEnv.csv is a data frame of environmental data for the same sites contained in the fish community data frame. Again, the first column contains site names from 1 to 30. The remaining columns contain measurements for 11 abiotic variables. Note that most functions for ordination analyses expect data to be in wide format. # Make sure the files are in your working directory! If R # cannot find the dataset, set your working directory with # setwd() to the folder in which your data is stored (e.g. # setwd(&#39;~/Desktop/workshop10&#39;)) # Species community data frame (fish abundance) spe &lt;- read.csv(&quot;data/doubsspe.csv&quot;, row.names = 1) spe &lt;- spe[-8, ] # Site number 8 contains no species, so we remove row 8 (site 8) # Be careful to only run this command line once as you are # overwriting &#39;spe&#39; each time! # Environmental data frame: “DoubsEnv.csv” env &lt;- read.csv(&quot;data/doubsenv.csv&quot;, row.names = 1) env &lt;- env[-8, ] # Remove corresponding abiotic data for site 8 (because removed from fish data). # Again, be careful to only run the last line once. 4.1 Exploring the fish community dataset We can begin by using summary functions to explore the spe data (fish community data), and get familiar with its dimensions, structure, column headings and some summary statistics. This is a review from Workshop 2. We can begin by getting a general overview of the matrix: names(spe) # names of objects (species) ## [1] &quot;CHA&quot; &quot;TRU&quot; &quot;VAI&quot; &quot;LOC&quot; &quot;OMB&quot; &quot;BLA&quot; &quot;HOT&quot; &quot;TOX&quot; &quot;VAN&quot; &quot;CHE&quot; &quot;BAR&quot; &quot;SPI&quot; ## [13] &quot;GOU&quot; &quot;BRO&quot; &quot;PER&quot; &quot;BOU&quot; &quot;PSO&quot; &quot;ROT&quot; &quot;CAR&quot; &quot;TAN&quot; &quot;BCO&quot; &quot;PCH&quot; &quot;GRE&quot; &quot;GAR&quot; ## [25] &quot;BBO&quot; &quot;ABL&quot; &quot;ANG&quot; dim(spe) # dataset dimensions ## [1] 29 27 head(spe) # look at first 5 rows ## CHA TRU VAI LOC OMB BLA HOT TOX VAN CHE BAR SPI GOU BRO PER BOU PSO ROT CAR ## 1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 2 0 5 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 5 5 5 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## 4 0 4 5 5 0 0 0 0 0 1 0 0 1 2 2 0 0 0 0 ## 5 0 2 3 2 0 0 0 0 5 2 0 0 2 4 4 0 0 2 0 ## 6 0 3 4 5 0 0 0 0 1 2 0 0 1 1 1 0 0 0 0 ## TAN BCO PCH GRE GAR BBO ABL ANG ## 1 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 ## 4 1 0 0 0 0 0 0 0 ## 5 3 0 0 0 5 0 0 0 ## 6 2 0 0 0 1 0 0 0 Then, we can look a little more closely at the objects in the matrix, which are the species in this case: str(spe) # structure of objects in dataset summary(spe) # summary statistics for all objects (min, mean, max, etc.) It is also a good idea to take a quick look at how the community is structured by plotting the distribution of species’ abundances in the dataset. # Count number of species frequencies in each abundance class ab &lt;- table(unlist(spe)) # Plot distribution of species frequencies barplot(ab, las = 1, # make axis labels perpendicular to axis xlab = &quot;Abundance class&quot;, ylab = &quot;Frequency&quot;, # label axes col = grey(5:0/5)) # 5-colour gradient for the bars You might notice that there are a lot of zeros in the abundance data. How many zeros are in the dataset? # Count the number of zeros in the dataset sum(spe == 0) ## [1] 408 What proportion of the dataset does that represent? # Calculate proportion of zeros in the dataset sum(spe == 0)/(nrow(spe) * ncol(spe)) ## [1] 0.5210728 Over 50% of our dataset consists of zeros! This is high, but not uncommon for species abundance data. However, many zeros can lead to a double zero problem, where common absences artificially increase the similarity between sites, in terms of their community composition. In other words, two sites might appear more similar just because they are both missing a species, even though a common absence does not make them ecologically similar. Instead, we want common presences to determine site similarity. To avoid this double zero problem, we will apply a transformation to the species data. Legendre and Gallagher (2001) proposed five pre-transformations of the species data, four of which are available in the decostand() function from the vegan package. The Hellinger transformation expresses abundances as the square-root of their relative abundance at each site (Borcard, Gillet, and Legendre 2011), solving the issue with double zeros. We will apply this transformation to the fish abundance dataset. # Apply Hellinger transformation to correct for the double # zero problem spe.hel &lt;- decostand(spe, method = &quot;hellinger&quot;) 4.2 Exploring the environmental dataset Now, let us get some familiarity with the abiotic environmental variables measured at the same sites. First, we can explore the matrix using the same function we used above. names(env) ## [1] &quot;das&quot; &quot;alt&quot; &quot;pen&quot; &quot;deb&quot; &quot;pH&quot; &quot;dur&quot; &quot;pho&quot; &quot;nit&quot; &quot;amm&quot; &quot;oxy&quot; &quot;dbo&quot; dim(env) ## [1] 29 11 head(env) ## das alt pen deb pH dur pho nit amm oxy dbo ## 1 0.3 934 48.0 0.84 7.9 45 0.01 0.20 0.00 12.2 2.7 ## 2 2.2 932 3.0 1.00 8.0 40 0.02 0.20 0.10 10.3 1.9 ## 3 10.2 914 3.7 1.80 8.3 52 0.05 0.22 0.05 10.5 3.5 ## 4 18.5 854 3.2 2.53 8.0 72 0.10 0.21 0.00 11.0 1.3 ## 5 21.5 849 2.3 2.64 8.1 84 0.38 0.52 0.20 8.0 6.2 ## 6 32.4 846 3.2 2.86 7.9 60 0.20 0.15 0.00 10.2 5.3 We can then look a little closer at the objects, which are the environmental variables in this case. str(env) summary(env) 4.2.1 Collinearity It is also a good idea to check for correlations between variables, as the constrained ordination methods we will be using are highly sensitive to collinearities in the explanatory matrix. This means a variable might appear to be highly important just because it was treated first in the analysis if it is highly correlated with one or more other variables that help to explain the response variable. # We can visually look for correlations between variables: heatmap(abs(cor(env)), # Compute pearson correlation (note they are absolute values) col = rev(heat.colors(6)), Colv = NA, Rowv = NA) legend(&quot;topright&quot;, title = &quot;Absolute Pearson R&quot;, legend = round(seq(0,1, length.out = 6),1), y.intersp = 0.7, bty = &quot;n&quot;, fill = rev(heat.colors(6))) Some variables look correlated… For example, das is highly correlated with alt, deb, dur, nit, among others! 4.2.2 Standardizing the environmental variables You cannot compare the effects of variables with different units. For example, a variable measured in millimeters would appear more important than if it were measured in meters, simple because the value is larger (e.g. 1000 millimeters vs. 1 meter). Standardizing variables with different units is therefore crucial. In this dataset, the environmental data are all in different units and will therefore need to be standardized prior to performing any ordinations. We can once again use the decostand() function to standardize the environmental variables. # Scale and center variables env.z &lt;- decostand(env, method = &quot;standardize&quot;) # Variables are now centered around a mean of 0 round(apply(env.z, 2, mean), 1) ## das alt pen deb pH dur pho nit amm oxy dbo ## 0 0 0 0 0 0 0 0 0 0 0 # and scaled to have a standard deviation of 1 apply(env.z, 2, sd) ## das alt pen deb pH dur pho nit amm oxy dbo ## 1 1 1 1 1 1 1 1 1 1 1 References "],["what-are-constrained-ordinations.html", "Chapter 5 What are “constrained” ordinations?", " Chapter 5 What are “constrained” ordinations? Described first by Rao (1964), canonical analysis is a generic term that for several types of statistical analyses sharing a common goal, which is to identify the relationship between a multivariate response table (matrix \\(Y\\), generally describing the species composition of communities) and a multivariate explanatory table (matrix \\(X\\), generally containing environmental descriptors) by combining ordination and regression concepts. Canonical analyses allow users to test ecological hypothesis concerning the environmental drivers of species composition. Among the diversity of canonical analysis, we will mainly focus here on Redundancy Analysis (RDA). References "],["redundancy-analysis.html", "Chapter 6 Redundancy analysis 6.1 How the RDA works 6.2 Running an RDA in R 6.3 Challenge 1", " Chapter 6 Redundancy analysis Redundancy Analysis (RDA) is a direct extension of multiple regression, as it models the effect of an explanatory matrix \\(X\\) (n x p) on a response matrix \\(Y\\) (n x m). The difference here is that we can model effect of an explanatory matrix on a response matrix, rather than a single response variable. For example, RDA allows us to model the effect of environmental variables on the entire community, rather than on species richness. This is done by performing an ordination of \\(Y\\) to obtain ordination axes that are linear combinations of the variables in \\(X\\). Figure 6.1: The basic structure of a redundancy analysis (RDA). Note that the explanatory variables in \\(X\\) can be quantitative, qualitative or binary variables. If they are quantitative, explanatory variables in \\(X\\) must be centered, standardized (if explanatory variables are in different units), transformed (to limit the skew of explanatory variables) or normalized (to linearize relationships) following the same principles as in PCA. Collinearity between the \\(X\\) variables should also be reduced before RDA. We began this process while exploring the data: our community data is Hellinger-transformed, and our environmental variables are centered and standardized. However, we still have some collinearity issues that have not been solved. Sometimes, we have more explanatory variables than we need to understand the drivers of our response variable. The best way to build a model is always to use ecological reasoning when determining which variables should be included or excluded. However, if there are still too many variables included in the model, or if some of them are highly collinear, explanatory variables can be selected by forward, backward or stepwise selection that remove non-significant explanatory variables. However, this approach should always be taken after the ecological selection of variables, according to your knowledge of the system. We will see more about this later! 6.1 How the RDA works Redundancy analysis as a two-step process (Legendre and Legendre 2012). The first step is a multiple regression, where each object in \\(Y\\) is regressed on the explanatory variables in \\(X\\), which results in a matrix of fitted values \\(Y_{fit}\\). This step is calculated through the following linear equation: \\[Y_{fit} = X[X&#39;X]^{-1}X&#39;Y\\] In the second step, we apply a principal components analysis (PCA) on the fitted matrix \\(Y_{fit}\\) to reduce dimensionality, i.e. to obtain the eigenvalues and eigenvectors. We then obtain a matrix \\(Z\\) which contains the canonical axes, which correspond to linear combinations of the explanatory variables in the space of \\(X\\). The linearity of the combinations of the \\(X\\) variables is a fundamental property of RDA. In the analysis of community composition, these canonical axes are interpreted as complex environmental gradients. Figure 6.2: The computation process of an RDA, from Legendre &amp; Legendre (2012). Once the RDA is computed, several statistics can be computed to interpret the explanatory power of the included variables and whether the observed relationships are significant. These include: \\(R^2\\), which measures the strength of the canonical relationship between \\(Y\\) and \\(X\\) by calculating the proportion of the variation of \\(Y\\) explained by the variables in \\(X\\), Adjusted \\(R^2\\), which also measures the strength of the relationship between \\(Y\\) and \\(X\\), but applies a correction of the \\(R^2\\) to take into account the number of explanatory variables. This is the statistic that should be reported. The F-statistic corresponds to an overall test of significance of an RDA by comparing the computed model to a null model. This test is based on the null hypothesis that the strength of the linear relationship calculated by the \\(R^2\\) is not larger than the value that would be obtained for unrelated \\(Y\\) and \\(X\\) matrices of the same size. Note that F-statistics can also be used to sequentially test the significance of each canonical axis. 6.2 Running an RDA in R An RDA can be computed using the function rda() from the vegan package, as follows: Step 1: Standardize and/or transform the data. We already applied a Hellinger transformation to our community matrix, and standardized our explanatory variables in the section: 4. However, we noticed that the variable das was collinear with several other variables. We will begin by removing this variable: # We&#39;ll use our standardized environmental data, but we will # remove &#39;das&#39;, which was correlated with many other # variables: env.z &lt;- subset(env.z, select = -das) Step 2: Run the RDA. # Model the effect of all environmental variables on fish # community composition spe.rda &lt;- rda(spe.hel ~ ., data = env.z) Step 3: Extract key results of the RDA. summary(spe.rda) The first section of the summary contains the pieces we need to verify the performance of our RDA. Let us break it down: ... ## Partitioning of variance: ## Inertia Proportion ## Total 0.5025 1.0000 ## Constrained 0.3689 0.7341 ## Unconstrained 0.1336 0.2659 ... Constrained Proportion: variance of \\(Y\\) explained by \\(X\\) (73.41%) Unconstrained Proportion: unexplained variance in \\(Y\\) (26.59%) How would you report these results? You could say: “The included environmental variables explain 73.41% of the variation in fish community composition across sites.” The rest of the RDA summary is not printed here, because it is long. Aside from the section printed above, the summary contains: Eigenvalues, and their contribution to the variance Accumulated constrained eigenvalues, including the cumulative proportion of explained variance by each axis in the final RDA ordination. These axes represent the rescaled environmental variables. If you need to select a subset of axes for other analyses, you can use this cumulative proportion to select the first few axes until you reach a threshold of your choice. Scores for species, sites, and the explanatory variables, which are the coordinates of each of these objects in the RDA space. The default scaling is of type 2 (we will come back to this). 6.2.1 Selecting variables If we want to simplify this model, we can perform a forward selection (or backwards or stepwise). These types of selections help us select variables that are statistically important. However, it is important to note that selecting variables ecologically is much more important than performing selection in this way. If a variable of ecological interest is not selected, this does not mean it has to be removed from the RDA. Here, we will be performing forward selection on our 11 environmental variables. To do this, we can use the ordiR2step() function (or using the forward.sel function of package packfor): # Forward selection of variables: fwd.sel &lt;- ordiR2step(rda(spe.hel ~ 1, data = env.z), # lower model limit (simple!) scope = formula(spe.rda), # upper model limit (the &quot;full&quot; model) direction = &quot;forward&quot;, R2scope = TRUE, # can&#39;t surpass the &quot;full&quot; model&#39;s R2 pstep = 1000, trace = FALSE) # change to TRUE to see the selection process! Here, we are essentially adding one variable at a time, and retaining it if it significantly increases the model’s adjusted \\(R^2\\). Which variables are retained by the forward selection? # Check the new model with forward-selected variables fwd.sel$call ## rda(formula = spe.hel ~ alt + oxy + dbo, data = env.z) What is the adjusted R2 of the RDA with the selected variables? # Write our new model spe.rda.signif &lt;- rda(spe.hel ~ alt + oxy + dbo, data = env.z) # check the adjusted R2 (corrected for the number of # explanatory variables) RsquareAdj(spe.rda.signif) ## $r.squared ## [1] 0.5894243 ## ## $adj.r.squared ## [1] 0.5401552 The explanatory variables (altitude, oxygen and biological oxygen demand) now explain 59% of the variance in \\(Y\\) (species abundances across sites, or community composition). When we correct for the number of variables in \\(X\\), the adjusted \\(R^2\\) tells us that three selected variables explain 54% of the variance in species abundances. Because the adjusted \\(R^2\\) is corrected for the number of explanatory variables, it is comparable across models and datasets. For this reason, you should report the adjusted \\(R^2\\) when writing up the result of an RDA for an article, or in a study which compares the explanatory power of different models. 6.2.2 Significance testing The significance of your RDA can be tested using the function anova.cca(). anova.cca(spe.rda.signif, step = 1000) ... ## Df Variance F Pr(&gt;F) ## Model 3 0.29619 11.963 0.001 *** ## Residual 25 0.20632 ## --- ... You can also test the significance of each variable with by = \"term\". anova.cca(spe.rda.signif, step = 1000, by = &quot;term&quot;) ... ## Model: rda(formula = spe.hel ~ alt + oxy + dbo, data = env.z) ## Df Variance F Pr(&gt;F) ## alt 1 0.164856 19.9759 0.001 *** ## oxy 1 0.082426 9.9877 0.001 *** ## dbo 1 0.048909 5.9264 0.001 *** ## Residual 25 0.206319 ... You can also test the significance of each canonical axis with by = \"axis\". Recall that these axes represent the variation in explanatory variables in fewer dimensions. anova.cca(spe.rda.signif, step = 1000, by = &quot;axis&quot;) ... ## Model: rda(formula = spe.hel ~ alt + oxy + dbo, data = env.z) ## Df Variance F Pr(&gt;F) ## RDA1 1 0.218022 26.4181 0.001 *** ## RDA2 1 0.050879 6.1651 0.001 *** ## RDA3 1 0.027291 3.3069 0.002 ** ## Residual 25 0.206319 ... Our full model is statistically significant (p = 0.001), and every variable included in this model is significant as well (p = 0.001). Every canonical axis resulting from the RDA is also statistically significant (p = 0.001). 6.2.3 RDA plot One of the most powerful aspects of RDA is the simultaneous visualization of your response and explanatory variables (i.e. species and environmental variables). As with the PCA in (Workshop 9)[https://github.com/QCBSRworkshops/workshop09], there are two types of scaling: Type 1 Type 2 Distances among objects reflect their similarities Angles between variables reflect their correlation # Type 1 scaling ordiplot(spe.rda.signif, scaling = 1, type = &quot;text&quot;) # Type 2 scaling ordiplot(spe.rda.signif, scaling = 2, type = &quot;text&quot;) Scaling 1 shows similarities between objects in the response matrix. Sites (numbers) that are closer together have more similar communities. Species that are closer together occupy more sites in common. Scaling 2 shows the effects of explanatory variables. Longer arrows mean this variable strongly drives the variation in the community matrix. Arrows pointing in opposite directions have a negative relationship. Arrows pointing in the same direction have a positive relationship. 6.2.3.1 Customizing RDA plots Both plot() and ordiplot() make quick and simple ordination plots, but you can customize your plots by extracting scores with scores() and manually setting the aesthetics of points(), text(), and arrows(). Here is an example of a custom triplot. Feel free to play around with the colours and other parameters to make it your own! ## extract % explained by the first 2 axes perc &lt;- round(100*(summary(spe.rda.signif)$cont$importance[2, 1:2]), 2) ## extract scores - these are coordinates in the RDA space sc_si &lt;- scores(spe.rda.signif, display=&quot;sites&quot;, choices=c(1,2), scaling=1) sc_sp &lt;- scores(spe.rda.signif, display=&quot;species&quot;, choices=c(1,2), scaling=1) sc_bp &lt;- scores(spe.rda.signif, display=&quot;bp&quot;, choices=c(1, 2), scaling=1) ## Custom triplot, step by step # Set up a blank plot with scaling, axes, and labels plot(spe.rda.signif, scaling = 1, # set scaling type type = &quot;none&quot;, # this excludes the plotting of any points from the results frame = FALSE, # set axis limits xlim = c(-1,1), ylim = c(-1,1), # label the plot (title, and axes) main = &quot;Triplot RDA - scaling 1&quot;, xlab = paste0(&quot;RDA1 (&quot;, perc[1], &quot;%)&quot;), ylab = paste0(&quot;RDA2 (&quot;, perc[2], &quot;%)&quot;) ) # add points for site scores points(sc_si, pch = 21, # set shape (here, circle with a fill colour) col = &quot;black&quot;, # outline colour bg = &quot;steelblue&quot;, # fill colour cex = 1.2) # size # add points for species scores points(sc_sp, pch = 22, # set shape (here, square with a fill colour) col = &quot;black&quot;, bg = &quot;#f2bd33&quot;, cex = 1.2) # add text labels for species abbreviations text(sc_sp + c(0.03, 0.09), # adjust text coordinates to avoid overlap with points labels = rownames(sc_sp), col = &quot;grey40&quot;, font = 2, # bold cex = 0.6) # add arrows for effects of the expanatory variables arrows(0,0, # start them from (0,0) sc_bp[,1], sc_bp[,2], # end them at the score value col = &quot;red&quot;, lwd = 3) # add text labels for arrows text(x = sc_bp[,1] -0.1, # adjust text coordinate to avoid overlap with arrow tip y = sc_bp[,2] - 0.03, labels = rownames(sc_bp), col = &quot;red&quot;, cex = 1, font = 2) 6.3 Challenge 1 Run an RDA to model the effects of environmental variables on mite species abundances. The mite dataset is part of the vegan package, so you do not need to have it stored as a .csv in your repository. To get started, load the mite data: # Load mite species abundance data data(&quot;mite&quot;) # Load environmental data data(&quot;mite.env&quot;) Recall some useful functions: decostand() rda() ordiR2step() anova.cca() ordiplot() 6.3.1 Challenge 1: Solution Step 1: Transform and standardize the data. # Hellinger transform the community data mite.spe.hel &lt;- decostand(mite, method = &quot;hellinger&quot;) # Standardize quantitative environmental data mite.env$SubsDens &lt;- decostand(mite.env$SubsDens, method = &quot;standardize&quot;) mite.env$WatrCont &lt;- decostand(mite.env$WatrCont, method = &quot;standardize&quot;) Step 2: Select environmental variables. # Initial RDA with ALL of the environmental data mite.spe.rda &lt;- rda(mite.spe.hel ~ ., data = mite.env) # Forward selection of environmental variables fwd.sel &lt;- ordiR2step(rda(mite.spe.hel ~ 1, data = mite.env), scope = formula(mite.spe.rda), direction = &quot;forward&quot;, R2scope = TRUE, pstep = 1000, trace = FALSE) fwd.sel$call ## rda(formula = mite.spe.hel ~ WatrCont + Shrub + Substrate + Topo, ## data = mite.env) Step 3: Run the RDA and check its explanatory power. # Re-run the RDA with the significant variables mite.spe.rda.signif &lt;- rda(mite.spe.hel ~ WatrCont + Shrub + Substrate + Topo + SubsDens, data = mite.env) # Find the adjusted R2 of the model with the retained env # variables RsquareAdj(mite.spe.rda.signif)$adj.r.squared ## [1] 0.4367038 Step 4: Test model significance. anova.cca(mite.spe.rda.signif, step = 1000) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = mite.spe.hel ~ WatrCont + Shrub + Substrate + Topo + SubsDens, data = mite.env) ## Df Variance F Pr(&gt;F) ## Model 11 0.20759 5.863 0.001 *** ## Residual 58 0.18669 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We find that four explanatory variables are retained after forward selection: WatrCont, Shrub, Substrate, and Topo. The selected environmental variables significantly explain 43.7% (p = 0.001) of the variation in mite species abundances. Step 5: Plot the RDA results! # Scaling 1 ordiplot(mite.spe.rda.signif, scaling = 1, main = &quot;Mite RDA - Scaling 1&quot;) # Scaling 2 ordiplot(mite.spe.rda.signif, scaling = 2, main = &quot;Mite RDA - Scaling 2&quot;) Scaling 1 shows similarities between objects in the response matrix. The sites (black circles) are overall similar A few species (red +) stand out from the cluster near the middle, meaning they do not occupy many sites in common with other species. These species might therefore be rare or unique in some ecological way. Scaling 2 shows the effects of explanatory variables. Substrate density and water content have long arrows, and therefore strong effects Shrubs have an opposite effect to WatrCont and SubsDens, because Shrub arrows are in opposite directions to these variables. Sites vary a lot in terms of SubsDens, and less in terms of WatrCont. References "],["partial-redundancy-analysis.html", "Chapter 7 Partial Redundancy Analysis 7.1 Example: Partial RDA on Doubs River data 7.2 Challenge 2", " Chapter 7 Partial Redundancy Analysis Partial RDA is a special case of RDA in which the response variables \\(Y\\) are related to explanatory variables \\(X\\) in the presence of additional explanatory variables \\(W\\), called covariates (or covariables). As in partial linear regression, the linear effect of \\(X\\) variables on the \\(Y\\) variables are adjusted for the effects of the covariates \\(W\\). For this, a RDA of the covariables \\(W\\) on the response variables \\(Y\\) is first performed. The residuals of this RDA are then extracted, i.e. a matrix \\(Y_{res}|W\\) containing the \\(Y\\) response variables in which the effect of \\(W\\) was removed. The partial RDA corresponds to the RDA of \\(X\\) on \\(Y_{res}|W\\). All statistics previously presented for RDA also apply for partial RDA. Figure 7.1: The basic structure of a redundancy analysis (RDA). Partial RDA has several applications. It is a powerful tool when users what to assess the effect of environmental variables on species composition while taking into account the variation due to other environmental variables that are not the focus of the study. A common example of this in community ecology is to test the importance of environmental variables while controlling for the effect of space. Partial RDA can also be used to control for well-known linear effects, to isolate the effect of a single explanatory variable, or to analyse related samples. 7.1 Example: Partial RDA on Doubs River data In R, a partial RDA is performed in the same way as the RDA we used previously, using rda(). As a demonstration, let’s assess the effect of water chemistry on fish species abundances (spe.hel) while controlling for the effect of topography. # Subset environmental data into topography variables and # chemistry variables env.topo &lt;- subset(env.z, select = c(alt, pen, deb)) env.chem &lt;- subset(env.z, select = c(pH, dur, pho, nit, amm, oxy, dbo)) # Run a partial RDA spe.partial.rda &lt;- rda(spe.hel, env.chem, env.topo) Note: You can also use a formula syntax like Y ~ X + Condition(W), where Condition() allows you to control for the covariates. # Alternative syntax for the partial RDA: spe.partial.rda &lt;- rda(spe.hel ~ pH + dur + pho + nit + amm + oxy + dbo + # these are the effects we are interested in Condition(alt + pen + deb), # these are the covariates data = env.z) 7.1.1 Interpreting partial RDA output in R The output of a partial RDA is very similar to the output discussed in the previous section on RDA. The key difference is that we have covariates in our model, which means we can see how much variation is explained by these additional, but not “interesting” variables. Once again, the first section of the summary contains the pieces we need to verify the performance of our partial RDA. Let us break it down: summary(spe.partial.rda) ... ## Partitioning of variance: ## Inertia Proportion ## Total 0.5025 1.0000 ## Conditioned 0.2087 0.4153 ## Constrained 0.1602 0.3189 ## Unconstrained 0.1336 0.2659 ... Conditioned Proportion: variance of \\(Y\\) explained by \\(W\\) (41.53%) Constrained Proportion: variance of \\(Y\\) explained by \\(X\\) .(31.89%) Unconstained Proportion: unexplained variance in \\(Y\\) (26.59%) How would you report these results? You could say something like: “Water chemistry explains 31.9% of the variation in fish community composition across sites, while topography explains 41.5% of this variation.” 7.1.2 Significance testing As with the RDA, we can interpret the significance of our model with two key pieces of information. What is the model’s explanatory power? # Extract the model&#39;s adjusted R2 RsquareAdj(spe.partial.rda)$adj.r.squared ## [1] 0.2413464 Is the model statistically significant? # Test whether the model is statistically significant anova.cca(spe.partial.rda, step = 1000) ... ## Permutation test for rda under reduced model ## Number of permutations: 999 ## ## Model: rda(X = spe.hel, Y = env.chem, Z = env.topo) ## Df Variance F Pr(&gt;F) ## Model 7 0.16024 3.0842 0.001 *** ## Residual 18 0.13360 ... Our model explains .alert[24.1%] of the variation in fish abundance across sites. It is also statistically significant (.alert[p = 0.001])! 7.1.3 Partial RDA plot We can visualise the effects of the environmental variables on the fish community with the ordiplot() function. ordiplot(spe.partial.rda, scaling = 2, main = &quot;Doubs River partial RDA - Scaling 2&quot;) Recall: Scaling 2 shows the effects of explanatory variables, meaning it shows the effects of the X matrix on the Y matrix (after the effect of matrix W has been controlled for). Note: The topography variables (covariates) aren’t plotted. Why is that? The partial RDA only adjusts the effects of the explanatory variables according to the covariates. The covariates are not of interest, and are therefore not plotted. 7.2 Challenge 2 Run a partial RDA to model the effects of environmental variables on mite species abundances (mite.spe.hel), while controlling for substrate variables (SubsDens, WatrCont, and Substrate). * What is the variance explained by substrate variables? * Is the model significant? * Which axes are significant? Recall some useful functions: rda() summary() RsquareAdj() anova.cca() # hint: see the &#39;by&#39; argument in ?anova.cca 7.2.1 Challenge 2: Solution Step 1: Transform and standardize the data. Our datasets have already been transformed and standardized. Step 2: Run a partial RDA. # Compute partial RDA mite.spe.subs &lt;- rda(mite.spe.hel ~ Shrub + Topo + Condition(SubsDens + WatrCont + Substrate), data = mite.env) # Check summary summary(mite.spe.subs) ... ## Partitioning of variance: ## Inertia Proportion ## Total 0.39428 1.00000 ## Conditioned 0.16891 0.42839 ## Constrained 0.03868 0.09811 ## Unconstrained 0.18669 0.47350 ... Shrub and Topo explain 9.8% of the variation in mite species abundances, while substrate covariables explain 42.8% of this variation. Step 3: Interpret the results! What is the variance explained by substrate variables? RsquareAdj(mite.spe.subs)$adj.r.squared ## [1] 0.08327533 Is the model significant? anova.cca(mite.spe.subs, step = 1000) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = mite.spe.hel ~ Shrub + Topo + Condition(SubsDens + WatrCont + Substrate), data = mite.env) ## Df Variance F Pr(&gt;F) ## Model 3 0.038683 4.006 0.001 *** ## Residual 58 0.186688 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Which axes are significant? anova.cca(mite.spe.subs, step = 1000, by = &quot;axis&quot;) ## Permutation test for rda under reduced model ## Forward tests for axes ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = mite.spe.hel ~ Shrub + Topo + Condition(SubsDens + WatrCont + Substrate), data = mite.env) ## Df Variance F Pr(&gt;F) ## RDA1 1 0.027236 8.4618 0.001 *** ## RDA2 1 0.008254 2.5643 0.019 * ## RDA3 1 0.003193 0.9919 0.430 ## Residual 58 0.186688 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The RDA’s adjusted \\(R^2\\) is 8.33%, and is significant (p = 0.001). Environmental variables explained 9.81% of the variance in mite species composition across sites, while substrate covariates explained 42.84% of this variation. Still, 47.35% of the variation is left unexplained. The first two canonical axes are significant. "],["variation-partitioning.html", "Chapter 8 Variation partitioning", " Chapter 8 Variation partitioning Variation partitioning is a type of analysis that combines RDA and partial RDA to divide the variation of a response variable among two, three or four explanatory data sets. Variation partitioning are generally represented by Venn diagram in which the percentage of explained variance by each explanatory data set (or combination of data stets) is reported. In the case of two datasets (below): - Fraction a + b +c is the explained variance by the two datasets calculated using a RDA of y by X + W. - Fraction d is the unexplained variance by the two datasets calculated using the same RDA as above. - Fraction a is the explained variance by the X data set only calculated using a partial of y by X with W as covariables. - Fraction c is the explained variance by the W data set only calculated using a partial of y by W with X as covariables. - Fraction b is calculated by subtraction, i.e. b = \\[a + b\\] + \\[b + c\\] - \\[a + b + c\\]. Venn diagram of partition of the variation of a response variable y among two sets of explanatory variables X and W (from Legendre and Legendre 2012). Variation partitioning is thus an indicated analysis when user what to relate the abundance of species in a community to various type of environmental variables, for example abiotic vs biotic variables, large-scale versus small-scale variables, etc. In the next example, we will partition the variation of fish species composition between chemical and physiographic variables. In R, variation partitioning is performed using the function varpart(). Venn diagrams can also be drawn using the function plot(). `?`(varpart) vegandocs(&quot;partitioning.pdf&quot;) # Variation partitioning with all explanatory variables spe.part.all &lt;- varpart(spe.hel, envchem, envtopo) spe.part.all windows(title = &quot;Variation partitioning - all variables&quot;) plot(spe.part.all, digits = 2) The output looks like: In this case, the chemical variables explain 24.10% of the fish species composition, the physiographic variables explain 11.20% of the fish species composition and the interaction of these two types of variables explained 23.30% of the fish species composition. Note that the varpart() function also identify the fractions that can be tested for significance using the function anova.cca(). Users can also perform variation partitioning between data sets that only contain significant environmental variables: # RDA of chemistry variables spe.chem &lt;- rda(spe.hel ~ ., data = envchem) # Select significant chemistry variables R2a.all.chem &lt;- RsquareAdj(spe.chem)$adj.r.squared ordiR2step(rda(spe.hel ~ 1, data = envchem), scope = formula(spe.chem), direction = &quot;forward&quot;, R2scope = TRUE, pstep = 1000) names(envchem) (envchem.pars &lt;- envchem[, c(4, 6, 7)]) # RDA with other environmental variables spe.topo &lt;- rda(spe.hel ~ ., data = envtopo) R2a.all.topo &lt;- RsquareAdj(spe.topo)$adj.r.squared ordiR2step(rda(spe.hel ~ 1, data = envtopo), scope = formula(spe.topo), direction = &quot;forward&quot;, R2scope = TRUE, pstep = 1000) names(envtopo) envtopo.pars &lt;- envtopo[, c(1, 2)] # Varpart spe.part &lt;- varpart(spe.hel, envchem.pars, envtopo.pars) windows(title = &quot;Variation partitioning - parsimonious subsets&quot;) plot(spe.part, digits = 2) # Tests of significance anova.cca(rda(spe.hel, envchem.pars), step = 1000) # Test of fractions [a+b] anova.cca(rda(spe.hel, envtopo.pars), step = 1000) # Test of fractions [b+c] env.pars &lt;- cbind(envchem.pars, envtopo.pars) anova.cca(rda(spe.hel, env.pars), step = 1000) # Test of fractions [a+b+c] anova.cca(rda(spe.hel, envchem.pars, envtopo.pars), step = 1000) # Test of fraction [a] anova.cca(rda(spe.hel, envtopo.pars, envchem.pars), step = 1000) # Test of fraction [c] Now, the chemical variables explain 25.30% of the fish species composition, the physiographic variables explain 14.20% of the fish species composition and the interaction of these two types of variables explained 19.60% of the fish species composition. All these fractions are significant (p&lt;0.001). Challenge 3: Perform variation partitioning of the mite species abundances with a first dataset for the significant substrate variables (SubsDens, WaterCont and Substrate) and a second dataset for the significant other variables (Shrud and Topo). What proportion of the variation are explained by each dataset? What are the significant fractions ? Challenge 3: Solution This is what your code may look like: str(mite.env) (mite.subs = mite.env[, c(1, 2, 3)]) #First set of variables outlined in challenge (mite.other = mite.env[, c(4, 5)]) #Second set of variables outlined in challenge # RDA for mite.subs rda.mite.subs &lt;- rda(mite.spe.hel ~ ., data = mite.subs) R2a.all.subs &lt;- RsquareAdj(rda.mite.subs)$adj.r.squared # Forward selection for mite.subs ordiR2step(rda(mite.spe.hel ~ 1, data = mite.subs), scope = formula(rda.mite.subs), direction = &quot;forward&quot;, R2scope = TRUE, pstep = 1000) names(mite.subs) (mite.subs.pars &lt;- mite.subs[, c(2, 3)]) # RDA for mite.other rda.mite.other &lt;- rda(mite.spe.hel ~ ., data = mite.other) R2a.all.other &lt;- RsquareAdj(rda.mite.other)$adj.r.squared # Forward selection for mite.other ordiR2step(rda(mite.spe.hel ~ 1, data = mite.other), scope = formula(rda.mite.other), direction = &quot;forward&quot;, R2scope = TRUE, pstep = 1000) names(mite.other) (mite.other.pars &lt;- mite.other[, c(1, 2)]) # Variation partitioning (mite.spe.part &lt;- varpart(mite.spe.hel, ~WatrCont + Substrate, ~Shrub + Topo, data = mite.env)) windows(title = &quot;Variation partitioning - parsimonious subsets&quot;) plot(mite.spe.part, digits = 2) # Tests of all testable fractions anova.cca(rda(mite.spe.hel ~ WatrCont + Substrate, data = mite.env), step = 1000) # Test of fractions [a+b] anova.cca(rda(mite.spe.hel ~ Shrub + Topo, data = mite.env), step = 1000) # Test of fractions [b+c] (env.pars &lt;- cbind(mite.env[, c(2, 3, 4, 5)])) anova.cca(rda(mite.spe.hel ~ WatrCont + Substrate + Shrub + Topo, data = env.pars), step = 1000) # Test of fractions [a+b+c] anova.cca(rda(mite.spe.hel ~ WatrCont + Substrate + Condition(Shrub + Topo), data = env.pars), step = 1000) # Test of fraction [a] anova.cca(rda(mite.spe.hel ~ Shrub + Topo + Condition(WatrCont + Substrate), data = env.pars), step = 1000) # Test of fraction [c] In this case, substrate variables explain 14.00% of the mite species composition, the other environmental variables explain 9.1% of the mite species composition and the interaction of these two types of vriables explained 16.90% of the mite species composition. All these fractions are significant (p&lt;0.001). "],["multivariate-regression-tree.html", "Chapter 9 Multivariate regression tree", " Chapter 9 Multivariate regression tree Multivariate regression tree (MRT) is a constrained clustering technique. Introduced by De’ath (2002), MRTs allow the partitioning of a quantitative response matrix by a matrix of explanatory variables constraining (guiding) on where to divide the data of the response matrix. RDA and MRT are both regression techniques, the former explaining the global structure of relationships through a linear model, the latter better highlighting local structures and interactions among variables by producing a tree model. Advantages of the MRT compared to the RDA: * does not make assumptions about the shape of the relationships between species and environmental variables (quantitative or categorical), * is robust in dealing with missing values * is robust in dealing with collinearity among the explanatory variables * is insensitive to transformations of the explanatory variables, which allows the use of raw values * the outcome, the tree, is easy to interpret, especially to a non-scientist audience. The MRT technique splits the data into clusters of samples similar in their species composition based on environmental value thresholds. It involves two procedures running at the same time: 1) the computation of the constrained partitioning of the data, and 2) the calculation of the relative error of the successive partitioning levels by multiple cross-validations. The function mvpart() from the package mvpart computes both the partition and the cross-validation. A quick note on MRT terminology: Leaf: Terminal group of sites Node: Point where the data splits into two groups. It is characterized by a threshold value of an explanatory variable. Branch: Each group formed by a split 1- Constrained partitioning of the data First, the method computes all possible partitions of the sites into two groups. For each quantitative explanatory variable, the sites will be sorted in the ascending values of the variables; for categorical variables, the sites will be aggregated by levels to test all combinations of levels. The method will split the data after the first object, the second object and so on, and compute the sum of within-group sum of squared distances to the group mean (within-group SS) for the response data. The method will retain the partition into two groups minimizing the within-group SS and the threshold value/level of the explanatory variable. These steps will be repeated within the two subgroups formed previously, until all objects form their own group. In other words, when each leaf of the tree contains one object. 2- Cross-validation and pruning the tree The mvpart function also performs a cross-validation and identifies the best predictive tree. The cross-validation procedure consists in using a subset of the objects to construct the tree, and to allocate the remaining objects to the groups. In a good predictive tree, objects are assigned to the appropriate groups. The cross-validated relative error (CVRE) is the measure of the predictive error. Without cross-validation, one would retain the number of partitions minimizing the variance not explained by the tree (i.e. the relative error: the sum of the within-group SS over all leaves divided by the overall SS of the data). This is the solution maximizing the R2 so to speak. This approach is explanatory rather than predictive. Let’s create a multivariate regression tree on the Doubs data. `?`(mvpart) # Prepare the data: remove “distance from source” env &lt;- subset(env, select = -das) # Create the regression tree doubs.mrt &lt;- mvpart(as.matrix(spe.hel) ~ ., env, legend = FALSE, margin = 0.01, cp = 0, xv = &quot;pick&quot;, xval = nrow(spe.hel), xvmult = 100, which = 4) At this point, you will need to select the tree who’s size (number of groups) is appropriate to the aim of your study from the following graph. This step requires the argument xv=\"pick\". In other words, you must prune the tree by picking the best-fit tree. Indeed, a fully resolved tree is not the desirable outcome. Instead, one is usually interested in a tree including only informative partitions/groups. It is possible to have an a-priori idea of the number of potential groups to be retained as well. The graph shows the relative error RE (in green) and the cross-validated relative error CVRE (in blue) of trees of increasing size. The red dot indicates the solution with the smallest CVRE, and the orange dot shows the smallest tree within one standard error of CVRE. It has been suggested that instead of choosing the solution minimizing CVRE, it would be more parsimonious to opt for the smallest tree for which the CVRE is within one standard error of the tree with the lowest CVRE (Breiman et al. 1984). The green bars at the top indicate the number of times each size was chosen during the cross-validation process. This graph is interactive, which means you will have to click on the blue point corresponding your choice of tree size. Once you do so, the corresponding multivariate regression tree will appear. If you click on the orange dot, the following tree appears. The statistics at the bottom of the figure are: the residual error (the reciprocal of the R2 of the model, in this case 43.7%), the cross-validated error, and the standard error. This tree has only two leaves separated by one node. This node splits the data into two groups at the threshold altitude value of 361.5m. Each leaf is characterized by a small barplot showing the abundances of the species, its number of sites and its relative error. We can compare this tree with the 10-group solution, as suggested by the CVRE criterion, or choose a solution in between, e.g. with 4 leaves to compare. # Using the CVRE criterion doubs.mrt.cvre &lt;- mvpart(as.matrix(spe.hel) ~ ., env, legend = FALSE, margin = 0.01, cp = 0, xv = &quot;pick&quot;, xval = nrow(spe.hel), xvmult = 100, which = 4) # Choosing ourself the best number of partitions doubs.mrt.4 &lt;- mvpart(as.matrix(spe.hel) ~ ., env, legend = FALSE, margin = 0.01, cp = 0, xv = &quot;pick&quot;, xval = nrow(spe.hel), xvmult = 100, which = 4) The 10-group solution has a high EXPLANATORY power but its predictive power (indicated by the cross-validated error) is just slightly better than that of the 2-group solution. The 4-group solution seems to be a good compromise. More information can be obtained by looking at the summary output. summary(doubs.mrt) CP stands for “complexity parameter”, which is the equivalent of the variance explained by each node. The CP at nsplit 0 is the R2 of the whole tree. The summary then outlines, for each node, the best threshold values to split the data. While informative, this output is very dense. A more detailed and yet more manageable output can be generated by using the wrapper from the function MRT() of the MVPARTwrap package. Plus, this other function allows identification of discriminant species. # Find discriminant species with MRT results doubs.mrt.wrap &lt;- MRT(doubs.mrt, percent = 10, species = colnames(spe.hel)) summary(doubs.mrt.wrap) # Extract indval p-values doubs.mrt.indval &lt;- indval(spe.hel, doubs.mrt$where) doubs.mrt.indval$pval # Extract indicator species of each node, with its indval doubs.mrt.indval$maxcls[which(doubs.mrt.indval$pval &lt;= 0.05)] doubs.mrt.indval$indcls[which(doubs.mrt.indval$pval &lt;= 0.05)] The main discriminant species of the first split are TRU, VAI and ABL. TRU and VAI contribute highly to the left leaf, and ABL is the most indicative species of the sites at lower altitude (&lt;361.5m). This output also indicates which sites are included in each leaf. The second part of the code allows us to test the significance of the indicator value of each species through a permutation test. For each significant indicator species, we extracted the leaf number and the indicator value. In this particular case, TRU, VAI and LOC are all significant species of the left leaf, TRU having the highest indicator value (0.867). Challenge 4: Run the multivariate regression tree for the mite data. Select the minimum size of tree within one SE of the CVRE. What is the proportion of variance explained by this tree? How many leaves contain this tree? What are the discriminant species? Challenge 4 - Solution mite.mrt &lt;- mvpart(data.matrix(mite.spe.hel) ~ ., mite.env, legend = FALSE, margin = 0.01, cp = 0, xv = &quot;pick&quot;, xval = nrow(mite.spe.hel), xvmult = 100, which = 4) summary(mite.mrt) mite.mrt.wrap &lt;- MRT(mite.mrt, percent = 10, species = colnames(mite.spe.hel)) summary(mite.mrt.wrap) mite.mrt.indval &lt;- indval(mite.spe.hel, mite.mrt$where) mite.mrt.indval$pval mite.mrt.indval$maxcls[which(mite.mrt.indval$pval &lt;= 0.05)] mite.mrt.indval$indcls[which(mite.mrt.indval$pval &lt;= 0.05)] 25.6% of the variation in the mite species assemblage across sites is explained by the partition of the sites based on water content of the substrate (at 385.1 mg/l). LCIL is a discriminant species of sites with higher water content, and has an indicator value of 0.715. "],["linear-discriminant-analysis.html", "Chapter 10 Linear discriminant analysis", " Chapter 10 Linear discriminant analysis Linear discriminant analysis (LDA) is a constrained (canonical) technique that allows you to determine how well your independent set of variables explains an a priori grouping. This grouping may have been obtained from a previous clustering analysis (see Workshop 8) or from a hypothesis (e.g. grouping is based on sites at different latitudes or different treatments). An LDA can also be used to classify new data into these pre-determined groups. You can imagine some useful applications of this technique including assessing which population a fish should be classified in based on morphology or classifying whether a new paper is a freshwater, marine or terrestrial study based on the abstract of papers in those pre-determined biomes. LDA computes discriminant functions from standardized descriptors. These coefficients quantify the relative contributions of the (standardized) explanatory variables to the discrimination of objects. Identification functions can be computed from the original (not standardized) descriptors to classify new data into pre-determined groups. Let’s continue to work with the Doubs fish data. First we must ensure that the within-group covariance matrices of the explanatory variables are homogeneous – a condition necessary for the application of LDA. First we want to make an a priori classification that is independent from the environmental data set. We know that there is a general relationship that indicates environmental variables change with latitude (Budyko 1969). Here we will classify our Doubs fish sites based on latitude to determine how well the environmental factors explain our latitude grouping. Our groups are determined by simply dividing the range of latitudes equally into three groups and then assigning each site to a group depending on where they fall along the divided range. # load spatial data to determine groups spa &lt;- read.csv(&quot;http://www.davidzeleny.net/anadat-r/data-download/DoubsSpa.csv&quot;, row.names = 1) spa &lt;- spa[, -8] # View spatial data View(spa) # add site numbers numbers &lt;- (1:30) numbers &lt;- numbers[!numbers %in% 8] spa$site &lt;- numbers # make groups based on lattitude y&lt;82=group1, # 82&lt;y&lt;156=group2, y&gt;156=group3 spa.group &lt;- ddply(.data = spa, .variables = .(x, y, site), .fun = summarise, group = if (y &lt;= 82) 1 else if (y &lt;= 156) 2 else 3) # order by site spa.group &lt;- spa.group[with(spa.group, order(site)), ] Generally, we would first want to check that the within-group covariance matrices of the explanatory variables are homogeneous by verifying multivariate homogeneity of within-group covariance (MHV). For the purposes of this workshop we will by pass it but more information can be found in Borcard et al. (2011). Once we run the LDA we can use the result object to determine 1. What groups the sites are classified in based on the environmental data. 2. What are the posterior probabilities of that the sites to belong to the groups. 3. The percentage of correct classification based on our latitudinal grouping. # run LDA LDA &lt;- lda(env, spa.group[, 4]) # classification of the objects based on LDA spe.class &lt;- predict(LDA)$class # posterior probabilities of the objects to belong to the # groups spe.post &lt;- predict(LDA)$posterior # table of prior versus predicted classifications spe.table &lt;- table(spa.group[, 4], spe.class) # proportion of correct classification diag(prop.table(spe.table, 1)) The results suggest that the environmental factors explain the first, lower latitude, group and the group 3 perfectly but only 83% of the group 2 sites were predicted correctly. What does that tell us about our classification? Perhaps there are stronger delineations in the lower and higher latitude and the group 2 is a mix of both? Now what we have some new sites and we want to classify them based on the relationship we have established between our latitudinal grouping and environmental factors using the LDA. Using the predict() function we can load in a new matrix with sites and classify them using the LDA object. Load in the classifyme.csv file, which contains dummy data from 5 new sites. # predicting classification of new data read in new sites classify.me &lt;- read.csv(&quot;classifyme.csv&quot;, header = T) # predict grouping of new data predict.group &lt;- predict(LDA, newdata = classify.me) # give classification for each new site group.new &lt;- predict.group$class Our new sites, in order, have been classified in groups 1,1, 1, 3 and 3 respectively. Challenge 5: Run an LDA for the mite env data (only first two vars) based on four latitudinal groups you create from the mite.xy data set. What group was group 2 most incorrectly grouped into? What proportion of sites was correctly classified in group 1? group 2? Challenge 5: Solution mite.xy$site &lt;- seq(1:70) (max(mite.xy[, 2]) - min(mite.xy[, 2]))/4 mite.xy.group &lt;- ddply(.data = mite.xy, .variables = .(x, y, site), .fun = summarise, group = if (y &lt;= 2.5) 1 else if (y &lt;= 4.9) 2 else if (y &lt;= 7.3) 3 else 4) mite.xy.group &lt;- mite.xy.group[with(mite.xy.group, order(site)), ] LDA.mite &lt;- lda(mite.env[, 1:2], mite.xy.group[, 4]) mite.class &lt;- predict(LDA.mite)$class mite.post &lt;- predict(LDA.mite)$posterior mite.table &lt;- table(mite.xy.group[, 4], mite.class) diag(prop.table(mite.table, 1)) "],["summary.html", "Chapter 11 Summary", " Chapter 11 Summary To be included. "],["additional-resources.html", "Chapter 12 Additional resources", " Chapter 12 Additional resources `?`(cca #(constrained correspondence analysis) ) # Constrained Correspondence Analysis (CCA) is a canonical # ordination method similar to RDA that preserve Chi-square # distances among object (instead of Euclidean distances in # RDA). This method is well suited for the analysis of large # ecological gradients. `?`(CCorA # Canonical Correlation Analysis ) # Canonical Correlation Analysis (CCorA) differs from RDA # given that the two matrices are considered symmetric while # in RDA the Y matrix is dependent on the X matrix. The main # use of this technique is to test the significance of the # correlation between two multidimensional data sets, then # explore the structure of the data by computing the # correlations (which are the square roots of the CCorA # eigenvalues) that can be found between linear functions of # two groups of descriptors. help(coinertia, package = ade4) # Coinertia Analysis # Coinertia Analysis (CoIA) is a symmetric canonical # ordination method that is appropriate to compare pairs of # data sets that play equivalent roles in the analysis. The # method finds a common space onto which the objects and # variables of these data sets can be projected and compared. # Compared to CCorA, co-inertia analysis imposes no # constraint regarding the number of variables in the two # sets, so that it can be used to compare ecological # communities even when they are species-rich. Co-inertia # analysis is not well-suited, however, to analyse pairs of # data sets that contain the same variables, because the # analysis does not establish one-to-one correspondences # between variables in the two data sets; the method does not # ‘know’ that the first variable is the same in the first and # the second data sets, and likewise for the other variables. help(mfa, package = ade4) # Multiple Factorial Analysis # Multiple factor analysis (MFA) can be used to compare # several data sets describing the same objects. MFA consists # in projecting objects and variables of two or more data # sets on a global PCA, computed from all data sets, in which # the sets receive equal weights. # Spatial analysis can be performed using packages AEM and # PCNM : http://r-forge.r-project.org/R/?group_id=195 Add the references below to the BibTeX file. Alday &amp; Marrs (2014). A simple test for alternative states in ecological restoration: the use of principal response curves. Journal of Vegetation Science, 17, 302-311. Borcard, Gillet &amp; Legendre (2011). Numerical Ecology with R. Springer New York. Breiman, L., J. H. Friedman, et al. (1984). Classification and Regression Trees. Belmont, California, USA, Wadsworth International Group. Budyko, M.I. (1969) The effect of solar radiation variations on the climate of the Earth. Tellus, 21(5), 611-619. Clarke &amp; Warwick (2001). Change in Marine Communities: An Approach to Statistical Analysis and Interpretation 2nd edition. Primer-E Ltd. De’ath, G. (2002). Multivariate regression trees : a new technique for modeling species-environment relationships. Ecology, 83(4), 1105–1117. Gotelli &amp; Ellison (2004). A Primer of Ecological Statistics. Sinaeuer Associates Inc., Sunderland MA. Legendre &amp; Legendre (2012). Numerical Ecology 3rd edition. Elsevier Science BV, Amsterdam. Poulin, Andersen &amp; Rochefort (2013) A new approach for tracking vegetation change after restoration: a case study with peatlands. Restoration Ecology, 21, 363-371. "],["references.html", "Chapter 13 References", " Chapter 13 References "]]
