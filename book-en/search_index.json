[["index.html", "Workshop 10: Advanced Multivariate Analyses in R QCBS R Workshop Series Preface 0.1 Code of conduct 0.2 Contributors 0.3 Contributing", " Workshop 10: Advanced Multivariate Analyses in R QCBS R Workshop Series Developed and maintained by the contributors of the QCBS R Workshop Series1 2023-04-25 18:08:22 Preface The QCBS R Workshop Series is a series of 10 workshops that walks participants through the steps required to use R for a wide array of statistical analyses relevant to research in biology and ecology. These open-access workshops were created by members of the QCBS both for members of the QCBS and the larger community. The content of this workshop has been peer-reviewed by several QCBS members. If you would like to suggest modifications, please contact the current series coordinators, listed on the main Github page. 0.1 Code of conduct The QCBS R Workshop Series and the QCBS R Symposium are venues dedicated to providing a welcoming and supportive environment for all people, regardless of background or identity. Participants, presenters and organizers of the workshop series and other related activities accept this Code of Conduct when being present at any workshop-related activities. We do not tolerate behaviour that is disrespectful or that excludes, intimidates, or causes discomfort to others. We do not tolerate discrimination or harassment based on characteristics that include, but are not limited to, gender identity and expression, sexual orientation, disability, physical appearance, body size, citizenship, nationality, ethnic or social origin, pregnancy, familial status, genetic information, religion or belief (or lack thereof), membership of a national minority, property, age, education, socio-economic status, technical choices, and experience level. It applies to all spaces managed by or affiliated with the workshop, including, but not limited to, workshops, email lists, and online forums such as GitHub, Slack and Twitter. 0.1.1 Expected behaviour All participants are expected to show respect and courtesy to others. All interactions should be professional regardless of platform: either online or in-person. In order to foster a positive and professional learning environment we encourage the following kinds of behaviours in all workshop events and platforms: Use welcoming and inclusive language Be respectful of different viewpoints and experiences Gracefully accept constructive criticism Focus on what is best for the community Show courtesy and respect towards other community members 0.1.2 Unacceptable behaviour Examples of unacceptable behaviour by participants at any workshop event/platform include: written or verbal comments which have the effect of excluding people on the - basis of membership of any specific group; causing someone to fear for their safety, such as through stalking or intimidation; violent threats or language directed against another person; the display of sexual or violent images; unwelcome sexual attention; nonconsensual or unwelcome physical contact; insults or put-downs; sexist, racist, homophobic, transphobic, ableist, or exclusionary jokes; incitement to violence, suicide, or self-harm; continuing to initiate interaction (including photography or recording) with - someone after being asked to stop; publication of private communication without consent. 0.2 Contributors Originally developed by: Contributed with changes to the presentation: Contributed with changes to the written material: Contributed by reporting issues and suggesting modifications: 0.3 Contributing Under construction. The QCBS R Workshop Series is part of the Québec Centre for Biodiversity Science, and is maintained by the series coordinators and graduent student, postdoctoral, and research professional members. The contributors for this workshop can be accessed here.↩︎ "],["learning-objectives.html", "Chapter 1 Learning objectives", " Chapter 1 Learning objectives In this workshop, you will learn how to perform advanced multivariate analyses on community data. This workshop concentrates on constrained methods such as redundancy analysis (RDA), multivariate regression tree (MRT) and linear discriminant analysis (LDA) to explore how environmental variables may be driving patterns in species assemblage across sites. "],["preparing-for-the-workshop.html", "Chapter 2 Preparing for the workshop", " Chapter 2 Preparing for the workshop All workshop materials are found at github.com/QCBSRworkshops/workshop10. This includes an R script which contains all code chunks shown in this book. For this workshop, we will be working with the following datasets: DoubsEnv DoubsSpe DoubsSpa Test data for linear discriminant analyses To download this data, do right click + save on the page that opens. You should also make sure you have downloaded, installed, and loaded these packages: vegan (for multivariate analyses) labdsv (for identification of significant indicator species in the multivariate regression tree analysis) MASS (for linear discriminant analysis) mvpart* (for multivariate regression trees) ggplot2 (for plotting some results) # Install the required packages install.packages(&quot;vegan&quot;) install.packages(&quot;labdsv&quot;) install.packages(&quot;MASS&quot;) install.packages(&quot;ggplot2&quot;) # install mvpart from package archive file install.packages(&quot;remotes&quot;) remotes::install_url(&quot;https://cran.r-project.org/src/contrib/Archive/mvpart/mvpart_1.6-2.tar.gz&quot;) *The mvpart package is no longer hosted on CRAN, but is still available from the archives. To install mvpart, you can also download the .tar.gz version from here and go to the “Packages” tab on the bottom right panel of R Studio, and click on “Install Packages”. Choose to install from Package Archive file, and select the .tar.gz for mvpart. # Load the required packages library(labdsv) library(vegan) library(MASS) library(mvpart) library(ggplot2) "],["why-advanced-multivariate-methods.html", "Chapter 3 Why “advanced multivariate methods”?", " Chapter 3 Why “advanced multivariate methods”? The previous workshop presented the basics of multivariate analyses: How to choose appropriate distance metrics and transformations Hierarchical clustering Unconstrained ordinations Principal component analysis Principal coordinate Analysis Correspondence analysis Nonmetric multidimensional scaling The present workshop builds on this knowledge, and will focus on constrained analyses. All the methods overviewed during Workshop 9 allowed us to find patterns in the community composition data or in the descriptors, but not to explore how environmental variables could be driving these patterns. With constrained analyses, such as redundancy analysis (RDA), linear discriminant analysis (LDA) and multivariate regression tree (MRT), one can describe and predict relationships between community composition data and environmental variables. "],["exploration.html", "Chapter 4 Exploring the Doubs River dataset 4.1 Exploring the fish community dataset 4.2 Exploring the environmental dataset", " Chapter 4 Exploring the Doubs River dataset We will be using the Doubs River dataset (Verneaux 1973) for this workshop. DoubsSpe.csv is a data frame of fish community data where the first column contains site names from 1 to 30 and the remaining columns are fish taxa (27 species). The taxa columns are populated by fish abundance data (counts). DoubsEnv.csv is a data frame of environmental data for the same sites contained in the fish community data frame. Again, the first column contains site names from 1 to 30. The remaining columns contain measurements for 11 abiotic variables. Note that most functions for ordination analyses expect data to be in wide format. # Make sure the files are in your working directory! If R # cannot find the dataset, set your working directory with # setwd() to the folder in which your data is stored (e.g. # setwd(&#39;~/Desktop/workshop10&#39;)) # Species community data frame (fish abundance) spe &lt;- read.csv(&quot;data/doubsspe.csv&quot;, row.names = 1) spe &lt;- spe[-8, ] # Site number 8 contains no species, so we remove row 8 (site 8) # Be careful to only run this command line once as you are # overwriting &#39;spe&#39; each time! # Environmental data frame: “DoubsEnv.csv” env &lt;- read.csv(&quot;data/doubsenv.csv&quot;, row.names = 1) env &lt;- env[-8, ] # Remove corresponding abiotic data for site 8 (because removed from fish data). # Again, be careful to only run the last line once. 4.1 Exploring the fish community dataset We can begin by using summary functions to explore the spe data (fish community data), and get familiar with its dimensions, structure, column headings and some summary statistics. This is a review from Workshop 2. We can begin by getting a general overview of the matrix: names(spe) # names of objects (species) ## [1] &quot;CHA&quot; &quot;TRU&quot; &quot;VAI&quot; &quot;LOC&quot; &quot;OMB&quot; &quot;BLA&quot; &quot;HOT&quot; &quot;TOX&quot; &quot;VAN&quot; &quot;CHE&quot; &quot;BAR&quot; &quot;SPI&quot; ## [13] &quot;GOU&quot; &quot;BRO&quot; &quot;PER&quot; &quot;BOU&quot; &quot;PSO&quot; &quot;ROT&quot; &quot;CAR&quot; &quot;TAN&quot; &quot;BCO&quot; &quot;PCH&quot; &quot;GRE&quot; &quot;GAR&quot; ## [25] &quot;BBO&quot; &quot;ABL&quot; &quot;ANG&quot; dim(spe) # dataset dimensions ## [1] 29 27 head(spe) # look at first 5 rows ## CHA TRU VAI LOC OMB BLA HOT TOX VAN CHE BAR SPI GOU BRO PER BOU PSO ROT CAR ## 1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 2 0 5 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 5 5 5 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## 4 0 4 5 5 0 0 0 0 0 1 0 0 1 2 2 0 0 0 0 ## 5 0 2 3 2 0 0 0 0 5 2 0 0 2 4 4 0 0 2 0 ## 6 0 3 4 5 0 0 0 0 1 2 0 0 1 1 1 0 0 0 0 ## TAN BCO PCH GRE GAR BBO ABL ANG ## 1 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 0 0 0 ## 4 1 0 0 0 0 0 0 0 ## 5 3 0 0 0 5 0 0 0 ## 6 2 0 0 0 1 0 0 0 Then, we can look a little more closely at the objects in the matrix, which are the species in this case: str(spe) # structure of objects in dataset summary(spe) # summary statistics for all objects (min, mean, max, etc.) It is also a good idea to take a quick look at how the community is structured by plotting the distribution of species’ abundances in the dataset. # Count number of species frequencies in each abundance class ab &lt;- table(unlist(spe)) # Plot distribution of species frequencies barplot(ab, las = 1, # make axis labels perpendicular to axis xlab = &quot;Abundance class&quot;, ylab = &quot;Frequency&quot;, # label axes col = grey(5:0/5)) # 5-colour gradient for the bars You might notice that there are a lot of zeros in the abundance data. How many zeros are in the dataset? # Count the number of zeros in the dataset sum(spe == 0) ## [1] 408 What proportion of the dataset does that represent? # Calculate proportion of zeros in the dataset sum(spe == 0)/(nrow(spe) * ncol(spe)) ## [1] 0.5210728 Over 50% of our dataset consists of zeros! This is high, but not uncommon for species abundance data. However, many zeros can lead to a double zero problem, where common absences artificially increase the similarity between sites, in terms of their community composition. In other words, two sites might appear more similar just because they are both missing some species, even though common absences do not make them ecologically similar. Instead, we want common presences to determine site similarity. To avoid this double zero problem, we will apply a transformation to the species data. Pierre Legendre and Gallagher (2001) proposed five pre-transformations of the species data, four of which are available in the decostand() function from the vegan package. The Hellinger transformation expresses abundances as the square-root of their relative abundance at each site (Borcard, Gillet, and Legendre 2011), solving the issue with double zeros. We will apply this transformation to the fish abundance dataset. # Apply Hellinger transformation to correct for the double # zero problem spe.hel &lt;- decostand(spe, method = &quot;hellinger&quot;) 4.2 Exploring the environmental dataset Now, let us get some familiarity with the abiotic environmental variables measured at the same sites. First, we can explore the matrix using the same function we used above. names(env) ## [1] &quot;das&quot; &quot;alt&quot; &quot;pen&quot; &quot;deb&quot; &quot;pH&quot; &quot;dur&quot; &quot;pho&quot; &quot;nit&quot; &quot;amm&quot; &quot;oxy&quot; &quot;dbo&quot; dim(env) ## [1] 29 11 head(env) ## das alt pen deb pH dur pho nit amm oxy dbo ## 1 0.3 934 48.0 0.84 7.9 45 0.01 0.20 0.00 12.2 2.7 ## 2 2.2 932 3.0 1.00 8.0 40 0.02 0.20 0.10 10.3 1.9 ## 3 10.2 914 3.7 1.80 8.3 52 0.05 0.22 0.05 10.5 3.5 ## 4 18.5 854 3.2 2.53 8.0 72 0.10 0.21 0.00 11.0 1.3 ## 5 21.5 849 2.3 2.64 8.1 84 0.38 0.52 0.20 8.0 6.2 ## 6 32.4 846 3.2 2.86 7.9 60 0.20 0.15 0.00 10.2 5.3 We can then look a little closer at the objects, which are the environmental variables in this case. str(env) summary(env) 4.2.1 Collinearity It is also a good idea to check for correlations between variables, as the constrained ordination methods we will be using are highly sensitive to collinearities in the explanatory matrix. This means a variable might appear to be highly important just because it was treated first in the analysis if it is highly correlated with one or more other variables that help to explain the response variable. # We can visually look for correlations between variables: heatmap(abs(cor(env)), # Compute pearson correlation (note they are absolute values) col = rev(heat.colors(6)), Colv = NA, Rowv = NA) legend(&quot;topright&quot;, title = &quot;Absolute Pearson R&quot;, legend = round(seq(0,1, length.out = 6),1), y.intersp = 0.7, bty = &quot;n&quot;, fill = rev(heat.colors(6))) Some variables look correlated… For example, das is highly correlated with alt, deb, dur, nit, among others! 4.2.2 Standardizing the environmental variables You cannot compare the effects of variables with different units. For example, a variable measured in millimeters would appear more important than if it were measured in meters, simple because the value is larger (e.g. 1000 millimeters vs. 1 meter). Standardizing variables with different units is therefore crucial. In this dataset, the environmental data are all in different units and will therefore need to be standardized prior to performing any ordinations. We can once again use the decostand() function to standardize the environmental variables. # Scale and center variables env.z &lt;- decostand(env, method = &quot;standardize&quot;) # Variables are now centered around a mean of 0 round(apply(env.z, 2, mean), 1) ## das alt pen deb pH dur pho nit amm oxy dbo ## 0 0 0 0 0 0 0 0 0 0 0 # and scaled to have a standard deviation of 1 apply(env.z, 2, sd) ## das alt pen deb pH dur pho nit amm oxy dbo ## 1 1 1 1 1 1 1 1 1 1 1 References "],["what-are-constrained-ordinations.html", "Chapter 5 What are “constrained” ordinations?", " Chapter 5 What are “constrained” ordinations? Described first by Rao (1964), canonical analysis is a generic term that for several types of statistical analyses sharing a common goal, which is to identify the relationship between a multivariate response table (matrix \\(Y\\), generally describing the species composition of communities) and a multivariate explanatory table (matrix \\(X\\), generally containing environmental descriptors) by combining ordination and regression concepts. Canonical analyses allow users to test ecological hypothesis concerning the environmental drivers of species composition. Among the diversity of canonical analysis, we will mainly focus here on Redundancy Analysis (RDA). References "],["redundancy-analysis.html", "Chapter 6 Redundancy analysis 6.1 How the RDA works 6.2 Running an RDA in R 6.3 Challenge 1", " Chapter 6 Redundancy analysis Redundancy Analysis (RDA) is a direct extension of multiple regression, as it models the effect of an explanatory matrix \\(X\\) (n x p) on a response matrix \\(Y\\) (n x m). The difference here is that we can model effect of an explanatory matrix on a response matrix, rather than a single response variable. For example, RDA allows us to model the effect of environmental variables on the entire community, rather than on species richness. This is done by performing an ordination of \\(Y\\) to obtain ordination axes that are linear combinations of the variables in \\(X\\). Figure 6.1: The basic structure of a redundancy analysis (RDA). Note that the explanatory variables in \\(X\\) can be quantitative, qualitative or binary variables. If they are quantitative, explanatory variables in \\(X\\) must be centered, standardized (if explanatory variables are in different units), transformed (to limit the skew of explanatory variables) or normalized (to linearize relationships) following the same principles as in PCA. Collinearity between the \\(X\\) variables should also be reduced before RDA. We began this process while exploring the data: our community data is Hellinger-transformed, and our environmental variables are centered and standardized. However, we still have some collinearity issues that have not been solved. Sometimes, we have more explanatory variables than we need to understand the drivers of our response variable. The best way to build a model is always to use ecological reasoning when determining which variables should be included or excluded. However, if there are still too many variables included in the model, or if some of them are highly collinear, explanatory variables can be selected by forward, backward or stepwise selection that remove non-significant explanatory variables. However, this approach should always be taken after the ecological selection of variables, according to your knowledge of the system. We will see more about this later! 6.1 How the RDA works Redundancy analysis as a two-step process (P. Legendre and Legendre 2012). The first step is a multiple regression, where each object in \\(Y\\) is regressed on the explanatory variables in \\(X\\), which results in a matrix of fitted values \\(Y_{fit}\\). This step is calculated through the following linear equation: \\[Y_{fit} = X[X&#39;X]^{-1}X&#39;Y\\] In the second step, we apply a principal components analysis (PCA) on the fitted matrix \\(Y_{fit}\\) to reduce dimensionality, i.e. to obtain the eigenvalues and eigenvectors. We then obtain a matrix \\(Z\\) which contains the canonical axes, which correspond to linear combinations of the explanatory variables in the space of \\(X\\). The linearity of the combinations of the \\(X\\) variables is a fundamental property of RDA. In the analysis of community composition, these canonical axes are interpreted as complex environmental gradients. Figure 6.2: The computation process of an RDA, from Legendre &amp; Legendre (2012). Once the RDA is computed, several statistics can be computed to interpret the explanatory power of the included variables and whether the observed relationships are significant. These include: \\(R^2\\), which measures the strength of the canonical relationship between \\(Y\\) and \\(X\\) by calculating the proportion of the variation of \\(Y\\) explained by the variables in \\(X\\), Adjusted \\(R^2\\), which also measures the strength of the relationship between \\(Y\\) and \\(X\\), but applies a correction of the \\(R^2\\) to take into account the number of explanatory variables. This is the statistic that should be reported. The F-statistic corresponds to an overall test of significance of an RDA by comparing the computed model to a null model. This test is based on the null hypothesis that the strength of the linear relationship calculated by the \\(R^2\\) is not larger than the value that would be obtained for unrelated \\(Y\\) and \\(X\\) matrices of the same size. Note that F-statistics can also be used to sequentially test the significance of each canonical axis. 6.2 Running an RDA in R An RDA can be computed using the function rda() from the vegan package, as follows: Step 1: Standardize and/or transform the data. We already applied a Hellinger transformation to our community matrix, and standardized our explanatory variables in the section: 4. However, we noticed that the variable das was collinear with several other variables. We will begin by removing this variable: # We&#39;ll use our standardized environmental data, but we # will remove &#39;das&#39;, which was correlated with many other # variables: env.z &lt;- subset(env.z, select = -das) Step 2: Run the RDA. # Model the effect of all environmental variables on fish # community composition spe.rda &lt;- rda(spe.hel ~ ., data = env.z) Step 3: Extract key results of the RDA. summary(spe.rda) The first section of the summary contains the pieces we need to verify the performance of our RDA. Let us break it down: ... ## Partitioning of variance: ## Inertia Proportion ## Total 0.5025 1.0000 ## Constrained 0.3689 0.7341 ## Unconstrained 0.1336 0.2659 ... Constrained Proportion: variance of \\(Y\\) explained by \\(X\\) (73.41%) Unconstrained Proportion: unexplained variance in \\(Y\\) (26.59%) How would you report these results? You could say: “The included environmental variables explain 73.41% of the variation in fish community composition across sites.” The rest of the RDA summary is not printed here, because it is long. Aside from the section printed above, the summary contains: Eigenvalues, and their contribution to the variance Accumulated constrained eigenvalues, including the cumulative proportion of explained variance by each axis in the final RDA ordination. These axes represent the rescaled environmental variables. If you need to select a subset of axes for other analyses, you can use this cumulative proportion to select the first few axes until you reach a threshold of your choice. Scores for species, sites, and the explanatory variables, which are the coordinates of each of these objects in the RDA space. The default scaling is of type 2 (we will come back to this). 6.2.1 Selecting variables If we want to simplify this model, we can perform a forward selection (or backwards or stepwise). These types of selections help us select variables that are statistically important. However, it is important to note that selecting variables ecologically is much more important than performing selection in this way. If a variable of ecological interest is not selected, this does not mean it has to be removed from the RDA. Here, we will be performing forward selection on our 11 environmental variables. To do this, we can use the ordiR2step() function (or using the forward.sel function of package packfor): # Forward selection of variables: fwd.sel &lt;- ordiR2step(rda(spe.hel ~ 1, data = env.z), # lower model limit (simple!) scope = formula(spe.rda), # upper model limit (the &quot;full&quot; model) direction = &quot;forward&quot;, R2scope = TRUE, # can&#39;t surpass the &quot;full&quot; model&#39;s R2 pstep = 1000, trace = FALSE) # change to TRUE to see the selection process! Here, we are essentially adding one variable at a time, and retaining it if it significantly increases the model’s adjusted \\(R^2\\). Which variables are retained by the forward selection? # Check the new model with forward-selected variables fwd.sel$call ## rda(formula = spe.hel ~ alt + oxy + dbo, data = env.z) What is the adjusted R2 of the RDA with the selected variables? # Write our new model spe.rda.signif &lt;- rda(spe.hel ~ alt + oxy + dbo, data = env.z) # check the adjusted R2 (corrected for the number of # explanatory variables) RsquareAdj(spe.rda.signif) ## $r.squared ## [1] 0.5894243 ## ## $adj.r.squared ## [1] 0.5401552 The explanatory variables (altitude, oxygen and biological oxygen demand) now explain 59% of the variance in \\(Y\\) (species abundances across sites, or community composition). When we correct for the number of variables in \\(X\\), the adjusted \\(R^2\\) tells us that three selected variables explain 54% of the variance in species abundances. Because the adjusted \\(R^2\\) is corrected for the number of explanatory variables, it is comparable across models and datasets. For this reason, you should report the adjusted \\(R^2\\) when writing up the result of an RDA for an article, or in a study which compares the explanatory power of different models. 6.2.2 Significance testing The significance of your RDA can be tested using the function anova.cca(). anova.cca(spe.rda.signif, step = 1000) ... ## Df Variance F Pr(&gt;F) ## Model 3 0.29619 11.963 0.001 *** ## Residual 25 0.20632 ## --- ... You can also test the significance of each variable with by = \"term\". anova.cca(spe.rda.signif, step = 1000, by = &quot;term&quot;) ... ## Model: rda(formula = spe.hel ~ alt + oxy + dbo, data = env.z) ## Df Variance F Pr(&gt;F) ## alt 1 0.164856 19.9759 0.001 *** ## oxy 1 0.082426 9.9877 0.001 *** ## dbo 1 0.048909 5.9264 0.002 ** ## Residual 25 0.206319 ... You can also test the significance of each canonical axis with by = \"axis\". Recall that these axes represent the variation in explanatory variables in fewer dimensions. anova.cca(spe.rda.signif, step = 1000, by = &quot;axis&quot;) ... ## Model: rda(formula = spe.hel ~ alt + oxy + dbo, data = env.z) ## Df Variance F Pr(&gt;F) ## RDA1 1 0.218022 26.4181 0.001 *** ## RDA2 1 0.050879 6.1651 0.001 *** ## RDA3 1 0.027291 3.3069 0.001 *** ## Residual 25 0.206319 ... Our full model is statistically significant (p = 0.001), and every variable included in this model is significant as well (p = 0.001). Every canonical axis resulting from the RDA is also statistically significant (p = 0.001). 6.2.3 RDA plot One of the most powerful aspects of RDA is the simultaneous visualization of your response and explanatory variables (i.e. species and environmental variables). As with the PCA in (Workshop 9)[https://github.com/QCBSRworkshops/workshop09], there are two types of scaling: Type 1 Type 2 Distances among objects reflect their similarities Angles between variables reflect their correlation # Type 1 scaling ordiplot(spe.rda.signif, scaling = 1, type = &quot;text&quot;) # Type 2 scaling ordiplot(spe.rda.signif, scaling = 2, type = &quot;text&quot;) Scaling 1 shows similarities between objects in the response matrix. Sites (numbers) that are closer together have more similar communities. Species that are closer together occupy more sites in common. Scaling 2 shows the effects of explanatory variables. Longer arrows mean this variable strongly drives the variation in the community matrix. Arrows pointing in opposite directions have a negative relationship. Arrows pointing in the same direction have a positive relationship. 6.2.3.1 Customizing RDA plots Both plot() and ordiplot() make quick and simple ordination plots, but you can customize your plots by extracting scores with scores() and manually setting the aesthetics of points(), text(), and arrows(). Here is an example of a custom triplot. Feel free to play around with the colours and other parameters to make it your own! # Custom triplot code! ## extract % explained by the first 2 axes perc &lt;- round(100*(summary(spe.rda.signif)$cont$importance[2, 1:2]), 2) ## extract scores - these are coordinates in the RDA space sc_si &lt;- scores(spe.rda.signif, display=&quot;sites&quot;, choices=c(1,2), scaling=1) sc_sp &lt;- scores(spe.rda.signif, display=&quot;species&quot;, choices=c(1,2), scaling=1) sc_bp &lt;- scores(spe.rda.signif, display=&quot;bp&quot;, choices=c(1, 2), scaling=1) ## Custom triplot, step by step # Set up a blank plot with scaling, axes, and labels plot(spe.rda.signif, scaling = 1, # set scaling type type = &quot;none&quot;, # this excludes the plotting of any points from the results frame = FALSE, # set axis limits xlim = c(-1,1), ylim = c(-1,1), # label the plot (title, and axes) main = &quot;Triplot RDA - scaling 1&quot;, xlab = paste0(&quot;RDA1 (&quot;, perc[1], &quot;%)&quot;), ylab = paste0(&quot;RDA2 (&quot;, perc[2], &quot;%)&quot;) ) # add points for site scores points(sc_si, pch = 21, # set shape (here, circle with a fill colour) col = &quot;black&quot;, # outline colour bg = &quot;steelblue&quot;, # fill colour cex = 1.2) # size # add points for species scores points(sc_sp, pch = 22, # set shape (here, square with a fill colour) col = &quot;black&quot;, bg = &quot;#f2bd33&quot;, cex = 1.2) # add text labels for species abbreviations text(sc_sp + c(0.03, 0.09), # adjust text coordinates to avoid overlap with points labels = rownames(sc_sp), col = &quot;grey40&quot;, font = 2, # bold cex = 0.6) # add arrows for effects of the expanatory variables arrows(0,0, # start them from (0,0) sc_bp[,1], sc_bp[,2], # end them at the score value col = &quot;red&quot;, lwd = 3) # add text labels for arrows text(x = sc_bp[,1] -0.1, # adjust text coordinate to avoid overlap with arrow tip y = sc_bp[,2] - 0.03, labels = rownames(sc_bp), col = &quot;red&quot;, cex = 1, font = 2) 6.3 Challenge 1 Run an RDA to model the effects of environmental variables on mite species abundances. The mite dataset is part of the vegan package, so you do not need to have it stored as a .csv in your repository. To get started, load the mite data: # Load mite species abundance data data(&quot;mite&quot;) # Load environmental data data(&quot;mite.env&quot;) Recall some useful functions: decostand() rda() ordiR2step() anova.cca() ordiplot() 6.3.1 Challenge 1: Solution Step 1: Transform and standardize the data. # Hellinger transform the community data mite.spe.hel &lt;- decostand(mite, method = &quot;hellinger&quot;) # Standardize quantitative environmental data mite.env$SubsDens &lt;- decostand(mite.env$SubsDens, method = &quot;standardize&quot;) mite.env$WatrCont &lt;- decostand(mite.env$WatrCont, method = &quot;standardize&quot;) Step 2: Select environmental variables. # Initial RDA with ALL of the environmental data mite.spe.rda &lt;- rda(mite.spe.hel ~ ., data = mite.env) # Forward selection of environmental variables fwd.sel &lt;- ordiR2step(rda(mite.spe.hel ~ 1, data = mite.env), scope = formula(mite.spe.rda), direction = &quot;forward&quot;, R2scope = TRUE, pstep = 1000, trace = FALSE) fwd.sel$call ## rda(formula = mite.spe.hel ~ WatrCont + Shrub + Substrate + Topo + ## SubsDens, data = mite.env) Step 3: Run the RDA and check its explanatory power. # Re-run the RDA with the significant variables mite.spe.rda.signif &lt;- rda(mite.spe.hel ~ WatrCont + Shrub + Substrate + Topo + SubsDens, data = mite.env) # Find the adjusted R2 of the model with the retained env # variables RsquareAdj(mite.spe.rda.signif)$adj.r.squared ## [1] 0.4367038 Step 4: Test model significance. anova.cca(mite.spe.rda.signif, step = 1000) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = mite.spe.hel ~ WatrCont + Shrub + Substrate + Topo + SubsDens, data = mite.env) ## Df Variance F Pr(&gt;F) ## Model 11 0.20759 5.863 0.001 *** ## Residual 58 0.18669 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We find that four explanatory variables are retained after forward selection: WatrCont, Shrub, Substrate, and Topo. The selected environmental variables significantly explain 43.7% (p = 0.001) of the variation in mite species abundances. Step 5: Plot the RDA results! # Scaling 1 ordiplot(mite.spe.rda.signif, scaling = 1, main = &quot;Mite RDA - Scaling 1&quot;) # Scaling 2 ordiplot(mite.spe.rda.signif, scaling = 2, main = &quot;Mite RDA - Scaling 2&quot;) Scaling 1 shows similarities between objects in the response matrix. The sites (black circles) are overall similar A few species (red +) stand out from the cluster near the middle, meaning they do not occupy many sites in common with other species. These species might therefore be rare or unique in some ecological way. Scaling 2 shows the effects of explanatory variables. Substrate density and water content have long arrows, and therefore strong effects Shrubs have an opposite effect to WatrCont and SubsDens, because Shrub arrows are in opposite directions to these variables. Sites vary a lot in terms of SubsDens, and less in terms of WatrCont. References "],["partial-redundancy-analysis.html", "Chapter 7 Partial Redundancy Analysis 7.1 Example: Partial RDA on Doubs River data 7.2 Challenge 2", " Chapter 7 Partial Redundancy Analysis Partial RDA is a special case of RDA in which the response variables \\(Y\\) are related to explanatory variables \\(X\\) in the presence of additional explanatory variables \\(W\\), called covariates (or covariables). As in partial linear regression, the linear effect of \\(X\\) variables on the \\(Y\\) variables are adjusted for the effects of the covariates \\(W\\). For this, a RDA of the covariables \\(W\\) on the response variables \\(Y\\) is first performed. The residuals of this RDA are then extracted, i.e. a matrix \\(Y_{res}|W\\) containing the \\(Y\\) response variables in which the effect of \\(W\\) was removed. The partial RDA corresponds to the RDA of \\(X\\) on \\(Y_{res}|W\\). All statistics previously presented for RDA also apply for partial RDA. Figure 7.1: The basic structure of a redundancy analysis (RDA). Partial RDA has several applications. It is a powerful tool when users what to assess the effect of environmental variables on species composition while taking into account the variation due to other environmental variables that are not the focus of the study. A common example of this in community ecology is to test the importance of environmental variables while controlling for the effect of space. Partial RDA can also be used to control for well-known linear effects, to isolate the effect of a single explanatory variable, or to analyse related samples. 7.1 Example: Partial RDA on Doubs River data In R, a partial RDA is performed in the same way as the RDA we used previously, using rda(). As a demonstration, let’s assess the effect of water chemistry on fish species abundances (spe.hel) while controlling for the effect of topography. # Subset environmental data into topography variables and # chemistry variables env.topo &lt;- subset(env.z, select = c(alt, pen, deb)) env.chem &lt;- subset(env.z, select = c(pH, dur, pho, nit, amm, oxy, dbo)) # Run a partial RDA spe.partial.rda &lt;- rda(spe.hel, env.chem, env.topo) Note: You can also use a formula syntax like Y ~ X + Condition(W), where Condition() allows you to control for the covariates. # Alternative syntax for the partial RDA: spe.partial.rda &lt;- rda(spe.hel ~ pH + dur + pho + nit + amm + oxy + dbo + # these are the effects we are interested in Condition(alt + pen + deb), # these are the covariates data = env.z) 7.1.1 Interpreting partial RDA output in R The output of a partial RDA is very similar to the output discussed in the previous section on RDA. The key difference is that we have covariates in our model, which means we can see how much variation is explained by these additional, but not “interesting” variables. Once again, the first section of the summary contains the pieces we need to verify the performance of our partial RDA. Let us break it down: summary(spe.partial.rda) ... ## Partitioning of variance: ## Inertia Proportion ## Total 0.5025 1.0000 ## Conditioned 0.2087 0.4153 ## Constrained 0.1602 0.3189 ## Unconstrained 0.1336 0.2659 ... Conditioned Proportion: variance of \\(Y\\) explained by \\(W\\) (41.53%) Constrained Proportion: variance of \\(Y\\) explained by \\(X\\) .(31.89%) Unconstained Proportion: unexplained variance in \\(Y\\) (26.59%) How would you report these results? You could say something like: “Water chemistry explains 31.9% of the variation in fish community composition across sites, while topography explains 41.5% of this variation.” 7.1.2 Significance testing As with the RDA, we can interpret the significance of our model with two key pieces of information. What is the model’s explanatory power? # Extract the model&#39;s adjusted R2 RsquareAdj(spe.partial.rda)$adj.r.squared ## [1] 0.2413464 Is the model statistically significant? # Test whether the model is statistically significant anova.cca(spe.partial.rda, step = 1000) ... ## Permutation test for rda under reduced model ## Number of permutations: 999 ## ## Model: rda(X = spe.hel, Y = env.chem, Z = env.topo) ## Df Variance F Pr(&gt;F) ## Model 7 0.16024 3.0842 0.001 *** ## Residual 18 0.13360 ... Our model explains 24.1% of the variation in fish abundance across sites. It is also statistically significant (p = 0.001)! 7.1.3 Partial RDA plot We can visualise the effects of the environmental variables on the fish community with the ordiplot() function. ordiplot(spe.partial.rda, scaling = 2, main = &quot;Doubs River partial RDA - Scaling 2&quot;) Recall: Scaling 2 shows the effects of explanatory variables, meaning it shows the effects of the X matrix on the Y matrix (after the effect of matrix W has been controlled for). Note: The topography variables (covariates) aren’t plotted. Why is that? The partial RDA only adjusts the effects of the explanatory variables according to the covariates. The covariates are not of interest, and are therefore not plotted. 7.2 Challenge 2 Run a partial RDA to model the effects of environmental variables on mite species abundances (mite.spe.hel), while controlling for substrate variables (SubsDens, WatrCont, and Substrate). * What is the variance explained by substrate variables? * Is the model significant? * Which axes are significant? Recall some useful functions: rda() summary() RsquareAdj() anova.cca() # hint: see the &#39;by&#39; argument in ?anova.cca 7.2.1 Challenge 2: Solution Step 1: Transform and standardize the data. Our datasets have already been transformed and standardized. Step 2: Run a partial RDA. # Compute partial RDA mite.spe.subs &lt;- rda(mite.spe.hel ~ Shrub + Topo + Condition(SubsDens + WatrCont + Substrate), data = mite.env) # Check summary summary(mite.spe.subs) ... ## Partitioning of variance: ## Inertia Proportion ## Total 0.39428 1.00000 ## Conditioned 0.16891 0.42839 ## Constrained 0.03868 0.09811 ## Unconstrained 0.18669 0.47350 ... Shrub and Topo explain 9.8% of the variation in mite species abundances, while substrate covariables explain 42.8% of this variation. Step 3: Interpret the results! What is the variance explained by substrate variables? RsquareAdj(mite.spe.subs)$adj.r.squared ## [1] 0.08327533 Is the model significant? anova.cca(mite.spe.subs, step = 1000) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = mite.spe.hel ~ Shrub + Topo + Condition(SubsDens + WatrCont + Substrate), data = mite.env) ## Df Variance F Pr(&gt;F) ## Model 3 0.038683 4.006 0.001 *** ## Residual 58 0.186688 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Which axes are significant? anova.cca(mite.spe.subs, step = 1000, by = &quot;axis&quot;) ## Permutation test for rda under reduced model ## Forward tests for axes ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = mite.spe.hel ~ Shrub + Topo + Condition(SubsDens + WatrCont + Substrate), data = mite.env) ## Df Variance F Pr(&gt;F) ## RDA1 1 0.027236 8.4618 0.001 *** ## RDA2 1 0.008254 2.5643 0.017 * ## RDA3 1 0.003193 0.9919 0.435 ## Residual 58 0.186688 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The RDA’s adjusted \\(R^2\\) is 8.33%, and is significant (p = 0.001). Environmental variables explained 9.81% of the variance in mite species composition across sites, while substrate covariates explained 42.84% of this variation. Still, 47.35% of the variation is left unexplained. The first two canonical axes are significant. "],["variation-partitioning.html", "Chapter 8 Variation partitioning 8.1 Variation partitioning in R 8.2 Significance testing 8.3 Challenge 3", " Chapter 8 Variation partitioning Variation partitioning is a type of analysis that combines RDA and partial RDA to divide the variation of a response variable among two, three or four explanatory data sets. For example, you might want to partition the variation in a community matrix among a set of abiotic environmental variables, and a set of biotic variables. You could also partition this community variation among small-scale or large-scale variables, to test the effect of spatial scale on your community. Figure 8.1: The basic structure of variation partitioning. The results of variation partitioning analyses are traditionally represented by a Venn diagram, in which the percentage of explained variance by each explanatory data set is reported. In a case where we are partitioning the variation among two explanatory matrices, the result could be represented as follows: Figure 8.2: Representing variation partitioning results. Here, Fraction \\([a + b + c]\\) is the explained variance by \\(X1\\) and* \\(X2\\) together, calculated using a RDA of \\(Y\\) by \\(X1 + X2\\). Fraction \\([d]\\) is the unexplained variance by \\(X1\\) and \\(X2\\) together, obtained from the same RDA as above. Fraction \\([a]\\) is the explained variance by \\(X1\\) only, calculated using a partial RDA of \\(Y\\) by \\(X1 | X2\\) (controlling for \\(X2\\)). Fraction \\([c]\\) is the explained variance by \\(X2\\) only, calculated using a partial RDA of \\(Y\\) by \\(X2 | X1\\) (controlling for \\(X1\\)). Fraction \\([b]\\) is calculated by subtraction, i.e. \\(b = [a + b] + [b + c] - [a + b + c].\\) Because \\([b]\\) is not the result of an RDA, it cannot be tested for significance. It can also be negative, which indicates that the response matrix is better explained by the combination of \\(X1\\) and \\(X2\\) than by either matrix on its own. 8.1 Variation partitioning in R To demonstrate how variation partitioning works in R, we will partition the variation of fish species composition between chemical and topographic variables. The varpart() function from vegan makes this easy for us. # Partition the variation in fish community composition spe.part.all &lt;- varpart(spe.hel, env.chem, env.topo) spe.part.all$part # access results! ## No. of explanatory tables: 2 ## Total variation (SS): 14.07 ## Variance: 0.50251 ## No. of observations: 29 ## ## Partition table: ## Df R.squared Adj.R.squared Testable ## [a+c] = X1 7 0.60579 0.47439 TRUE ## [b+c] = X2 3 0.41526 0.34509 TRUE ## [a+b+c] = X1+X2 10 0.73414 0.58644 TRUE ## Individual fractions ## [a] = X1|X2 7 0.24135 TRUE ## [b] = X2|X1 3 0.11205 TRUE ## [c] 0 0.23304 FALSE ## [d] = Residuals 0.41356 FALSE ## --- ## Use function &#39;rda&#39; to test significance of fractions of interest You can then visualise the results with the plot() function. # plot the variation partitioning Venn diagram plot(spe.part.all, Xnames = c(&quot;Chem&quot;, &quot;Topo&quot;), # name the partitions bg = c(&quot;seagreen3&quot;, &quot;mediumpurple&quot;), alpha = 80, # colour the circles digits = 2, # only show 2 digits cex = 1.5) The chemical variables explain 24.1% of the variation in fish species composition, the topography variables explain 11.2% of the variation in fish species composition, and these two variable groups jointly explain 23.3% of the variation in fish species composition. Be careful when reporting results of variation partitioning! The shared fraction [b] does not represent an interaction effect of the two explanatory matrices. Think of it as an overlap between \\(X1\\) and \\(X2\\). It represents the shared fraction of variation explained when the two are included in the model, meaning it is the portion of variation that cannot be attributed to \\(X1\\) or \\(X2\\) separately. In other words, the variation partitioning cannot disentangle the effects of chemistry and topography on 23.3% of the variation in the fish community composition. 8.2 Significance testing The output from the varpart() function reports the adjusted \\(R^2\\) for each fraction, but you will notice that the table does not include any test of statistical significance. However, the Testable column identifies the fractions that can be tested for significance using the function anova.cca(), just like we did with the RDA! X1 [a+b]: Chemistry without controlling for topography # [a+b] Chemistry without controlling for topography anova.cca(rda(spe.hel, env.chem)) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(X = spe.hel, Y = env.chem) ## Df Variance F Pr(&gt;F) ## Model 7 0.30442 4.6102 0.001 *** ## Residual 21 0.19809 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 X2 [b+c] Topography without controlling for chemistry # [b+c] Topography without controlling for chemistry anova.cca(rda(spe.hel, env.topo)) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(X = spe.hel, Y = env.topo) ## Df Variance F Pr(&gt;F) ## Model 3 0.20867 5.918 0.001 *** ## Residual 25 0.29384 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 X1 | X2 [a] Chemistry alone # [a] Chemistry alone anova.cca(rda(spe.hel, env.chem, env.topo)) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(X = spe.hel, Y = env.chem, Z = env.topo) ## Df Variance F Pr(&gt;F) ## Model 7 0.16024 3.0842 0.001 *** ## Residual 18 0.13360 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Recognize this? It’s a partial RDA! X2 | X1 [c] Topography alone # [c] Topography alone anova.cca(rda(spe.hel, env.topo, env.chem)) ## Permutation test for rda under reduced model ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(X = spe.hel, Y = env.topo, Z = env.chem) ## Df Variance F Pr(&gt;F) ## Model 3 0.064495 2.8965 0.001 *** ## Residual 18 0.133599 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 All of the testable fractions in the variation partitioning are statistically significant! 8.3 Challenge 3 Partition the variation in the mite species data according to substrate variables (SubsDens, WatrCont) and significant spatial variables. What proportion of the variation is explained by substrate variables? By space? Which individual fractions are significant? Plot your results! Load the spatial variables: data(&quot;mite.pcnm&quot;) Recall some useful functions: ordiR2step() varpart() anova.cca(rda()) plot() 8.3.1 Challenge 3: Solution Step 1: Forward selection of significant spatial variables. There are a lot of spatial variables in this dataset (22!). We should select the most important ones, to avoid overloading the model. # Step 1: Forward selection! # Write full RDA model with all variables full.spat &lt;- rda(mite.spe.hel ~ ., data = mite.pcnm) # Forward selection of spatial variables spat.sel &lt;- ordiR2step(rda(mite.spe.hel ~ 1, data = mite.pcnm), scope = formula(full.spat), R2scope = RsquareAdj(full.spat)$adj.r.squared, direction = &quot;forward&quot;, trace = FALSE) spat.sel$call ## rda(formula = mite.spe.hel ~ V2 + V3 + V8 + V1 + V6 + V4 + V9 + ## V16 + V7 + V20, data = mite.pcnm) Step 2: Group variables of interest. # Step 2: Group variables of interest. # Subset environmental data to retain only substrate # variables mite.subs &lt;- subset(mite.env, select = c(SubsDens, WatrCont)) # Subset to keep only selected spatial variables mite.spat &lt;- subset(mite.pcnm, select = names(spat.sel$terminfo$ordered)) # a faster way to access the selected variables! Step 3: Partition the variation in species abundances. # Step 3: Partition the variation in species abundances. mite.part &lt;- varpart(mite.spe.hel, mite.subs, mite.spat) mite.part$part$indfract # access results! ## Df R.squared Adj.R.squared Testable ## [a] = X1|X2 2 NA 0.05901929 TRUE ## [b] = X2|X1 10 NA 0.19415929 TRUE ## [c] 0 NA 0.24765221 FALSE ## [d] = Residuals NA NA 0.49916921 FALSE What proportion of the variation is explained by substrate variables? 5.9% What proportion of the variation is explained by spatial variables? 19.4% Step 4: Which individual fractions are significant? [a]: Substrate only # Step 4: Significance testing [a]: Substrate only anova.cca(rda(mite.spe.hel, mite.subs, mite.spat)) ... ## Model: rda(X = mite.spe.hel, Y = mite.subs, Z = mite.spat) ## Df Variance F Pr(&gt;F) ## Model 2 0.025602 4.4879 0.001 *** ## Residual 57 0.162583 ... [c]: Space only # [c]: Space only anova.cca(rda(mite.spe.hel, mite.spat, mite.subs)) ... ## Model: rda(X = mite.spe.hel, Y = mite.spat, Z = mite.subs) ## Df Variance F Pr(&gt;F) ## Model 10 0.10286 3.6061 0.001 *** ## Residual 57 0.16258 ... Step 5: Plot the variation partitioning results. # Step 5: Plot plot(mite.part, digits = 2, Xnames = c(&quot;Subs&quot;, &quot;Space&quot;), # label the fractions cex = 1.5, bg = c(&quot;seagreen3&quot;, &quot;mediumpurple&quot;), # add colour! alpha = 80) # adjust transparency So, what can we say about the effects of substrate and space on mite species abundances? Hint: Why is the model showing such an important effect of space? Space explains a lot of the variation in species abundances here: 19.4% (p = 0.001) of the variation is explained by space alone, and 24.8% is jointly explained by space and substrate. Substrate only explains ~6% (p = 0.001) of the variation in community composition across sites on its own! Also note that half of the variation is not explained by the variables we included in the model (look at the residuals!), so the model could be improved. This large effect of space could be a sign that some spatial ecological process is important here (like dispersal, for example). However, it could also be telling us that we are missing an important environmental variable in our model, which itself varies in space! "],["multivariate-regression-tree.html", "Chapter 9 Multivariate regression tree 9.1 Computing the MRT 9.2 MRT in R 9.3 Challenge 4", " Chapter 9 Multivariate regression tree Multivariate regression tree (MRT) is a hierarchical constrained clustering technique. Introduced by De’ath (2002), the MRT splits a response matrix (\\(Y\\)) into clusters based on thresholds of explanatory variables (\\(X\\)). Like RDA, MRT is a regression technique. While the former explains the global structure of relationships through a linear model, the latter produces a tree model to highlight local structures and interactions among variables. Figure 9.1: The basic structure of a multivariate regression tree (MRT). MRT has many convenient characteristics: It does not assume a linear relationship between Y and X matrices; The results are easy to visualize and interpret (it’s a tree!); It clearly identifies importance of explanatory variables; It is robust to missing values; It is robust to collinearity among the explanatory variables; It can handle raw explanatory variables, meaning there is no need to standardize. A quick note on MRT terminology: Branch: each group formed by a split; Node: splitting point (threshold value of an explanatory variable); Leaf: terminal group of sites. 9.1 Computing the MRT The MRT splits the data into clusters of samples similar in their species composition based on environmental value thresholds. It involves two procedures running at the same time: 1) the computation of the constrained partitioning of the data, and 2) the calculation of the relative error of the successive partitioning levels by multiple cross-validations. This cross-validation is, in essence, aiming to identify best predictive tree. The “best” tree varies depending on your study goals. Usually you want a tree that is parsimonious, but still has an informative number of groups. This is, of course, a subjective decision to make according to the question you are trying to answer. 9.1.1 Building the tree: Constrained partitioning of the data First, the method computes all possible partitions of the sites into two groups. For each quantitative explanatory variable, the sites will be sorted in the ascending values of the variables. For categorical variables, the sites will be aggregated by levels to test all combinations of levels. The method will split the data after the first object, the second object and so on, and compute the sum of within-group sum of squared distances to the group mean (within-group SS) for the response data. The method will retain the partition into two groups minimizing the within-group SS and the threshold value/level of the explanatory variable. These steps will be repeated within the two subgroups formed previously, until all objects form their own group. In other words, this process ends when each leaf of the tree contains one object. 9.1.2 Selecting the tree: Cross-validation and pruning The next step is to perform a cross-validation and identify the best predictive tree. The cross-validation procedure consists in using a subset of the objects to construct the tree, and to allocate the remaining objects to the groups. In a good predictive tree, objects are assigned to the appropriate groups. The cross-validated relative error (CVRE) is the measure of the predictive error. Without cross-validation, one would retain the number of partitions minimizing the variance not explained by the tree (i.e. the relative error: the sum of the within-group SS over all leaves divided by the overall SS of the data). This is the solution which maximizes the \\(R^2\\), so to speak. 9.2 MRT in R The function mvpart() from the package mvpart computes both the partition and the cross-validation steps required to build a multivariate regression tree. We will demonstrate the process of building a multivariate regression tree on the Doubs River data. # First, remove the &quot;distance from source&quot; variable env &lt;- subset(env, select = -das) # Create multivariate regression tree doubs.mrt &lt;- mvpart(as.matrix(spe.hel) ~ ., data = env, xv = &quot;pick&quot;, # interactively select best tree xval = nrow(spe.hel), # number of cross-validations xvmult = 100, # number of multiple cross-validations which = 4, # plot both node labels legend = FALSE, margin = 0.01, cp = 0) ## X-Val rep : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ## Minimum tree sizes ## tabmins ## 2 3 4 5 6 7 8 9 10 ## 7 1 2 4 11 9 23 4 39 At this point, you will need to select the tree with an appropriate number of groups, depending on the aim of your study. In other words, you must prune the tree by picking the best-fit tree. A fully resolved tree is not the desirable outcome; instead, one is usually interested in a tree including only informative partitions/groups. In such cases, it is possible to have an a priori idea of the number of potential groups to be retained. You can make this choice interactively, with the argument xv = \"pick\". The resulting figure shows the relative error RE (in green) and the cross-validated relative error CVRE (in blue) of trees of increasing size. The red dot indicates the solution with the smallest CVRE, and the orange dot shows the smallest tree within one standard error of CVRE. It has been suggested that instead of choosing the solution minimizing CVRE, it would be more parsimonious to opt for the smallest tree for which the CVRE is within one standard error of the tree with the lowest CVRE Breiman et al. (1984). The green bars at the top indicate the number of times each size was chosen during the cross-validation process. This graph is interactive, which means you will have to click on the blue point corresponding your choice of tree size. In summary: Green points: Relative error Blue points: Cross-validated relative error (CVRE) Red dot: Which tree has the smallest CVRE Orange dot: Smallest tree within one standard error of the CVRE Lime green bars: number of times each tree size was chosen We don’t have an a priori expectation about how to partition this data, so we’ll select the smallest tree within 1 standard error of the overall best-fit tree (i.e. the orange dot). We can select this tree using the xv = \"1se\" argument. # Select the solution we want doubs.mrt &lt;- mvpart(as.matrix(spe.hel) ~ ., data = env, xv = &quot;1se&quot;, # select smallest tree within 1 se xval = nrow(spe.hel), # number of cross-validations xvmult = 100, # number of multiple cross-validations which = 4, # plot both node labels legend = FALSE, margin = 0.01, cp = 0) ## X-Val rep : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ## Minimum tree sizes ## tabmins ## 2 3 5 6 7 8 9 10 ## 10 2 3 13 6 26 4 36 The statistics at the bottom of the figure are: the residual error, the cross-validated error, and the standard error. This tree has only two leaves separated by one node. Each leaf is characterized by a small barplot showing the abundances of the species included in the group, the number of sites in the group, and the group’s relative error. From this figure, we can report the following statistics: * The species matrix is partitioned according to an altitude threshold (361.5 m) * Residual error = 0.563, which means the model’s \\(R^2\\) is 43.7% (\\(1 - 0.563 = 0.437\\)) 9.2.1 MRT selection process We can also compare solutions, to help us chose the best tree. For example, let’s take a look at a 10-group solution! # Trying 10 groups mvpart(as.matrix(spe.hel) ~ ., data = env, xv = &quot;none&quot;, # no cross-validation size = 10, # set tree size which = 4, legend = FALSE, margin = 0.01, cp = 0, prn = FALSE) This tree is much harder to interpret, because there are so many groups! Although this version of the tree offers higher explanatory power, its predictive power (CV Error = 0.671) is basically the same as the previous two-group solution (CV Error = 0.673). This suggests that we may want to try a tree with a few more groupings than the two-group solution, while staying lower than 10 groups. Let’s look at a solution with fewer (4) groups! # Trying fewer groups mvpart(as.matrix(spe.hel) ~ ., data = env, xv = &quot;none&quot;, # no cross-validation size = 4, # set tree size which = 4, legend = FALSE, margin = 0.01, cp = 0, prn = FALSE) This tree is much easier to interpret! It also offers higher explanatory power (lower Error) than our original solution, and higher predictive power than both previous solutions (CV Error). We have a winner! 9.2.2 Interpreting MRT output To find out how much variance is explained by each node in the tree, we need to look at the complexity parameter (CP). The CP at nsplit = 0 is the \\(R^2\\) of the entire tree. # Checking the complexity parameter doubs.mrt$cptable ## CP nsplit rel error xerror xstd ## 1 0.4369561 0 1.0000000 1.0758122 0.07493568 ## 2 0.1044982 1 0.5630439 0.6755865 0.09492709 The summary then outlines, for each node, the best threshold values to split the data. # Checking the tree result summary summary(doubs.mrt) ## Call: ## mvpart(form = as.matrix(spe.hel) ~ ., data = env, xv = &quot;1se&quot;, ## xval = nrow(spe.hel), xvmult = 100, margin = 0.01, which = 4, ## legend = FALSE, cp = 0) ## n= 29 ## ## CP nsplit rel error xerror xstd ## 1 0.4369561 0 1.0000000 1.0758122 0.07493568 ## 2 0.1044982 1 0.5630439 0.6755865 0.09492709 ## ## Node number 1: 29 observations, complexity param=0.4369561 ## Means=0.07299,0.2472,0.2581,0.2721,0.07133,0.06813,0.06897,0.07664,0.1488,0.2331,0.113,0.07879,0.1724,0.1366,0.1103,0.08216,0.08751,0.07113,0.07312,0.1345,0.06307,0.04423,0.1015,0.1862,0.07713,0.1623,0.07283, Summed MSE=0.4851823 ## left son=2 (15 obs) right son=3 (14 obs) ## Primary splits: ## alt &lt; 361.5 to the right, improve=0.4369561, (0 missing) ## deb &lt; 23.65 to the left, improve=0.4369561, (0 missing) ## amm &lt; 0.06 to the left, improve=0.3529830, (0 missing) ## nit &lt; 1.415 to the left, improve=0.3513335, (0 missing) ## pen &lt; 1.5 to the right, improve=0.3372429, (0 missing) ## ## Node number 2: 15 observations ## Means=0.1208,0.4463,0.4194,0.4035,0.1104,0.09023,0,0.02108,0.1256,0.2164,0.04392,0.01054,0.107,0.09779,0.06853,0,0.01054,0.01617,0.01054,0.09489,0,0,0,0.08629,0,0,0, Summed MSE=0.3194207 ## ## Node number 3: 14 observations ## Means=0.02179,0.03391,0.08514,0.1313,0.02945,0.04444,0.1429,0.1362,0.1736,0.2509,0.1871,0.1519,0.2425,0.1781,0.1551,0.1702,0.17,0.13,0.1402,0.177,0.1306,0.09163,0.2103,0.2932,0.1598,0.3362,0.1509, Summed MSE=0.2236343 9.2.3 Indicator species You might also be interested in finding out which species are significant indicator species for each grouping of sites. # Calculate indicator values (indval) for each species doubs.mrt.indval &lt;- indval(spe.hel, doubs.mrt$where) # Extract the significant indicator species (and which node # they represent) doubs.mrt.indval$maxcls[which(doubs.mrt.indval$pval &lt;= 0.05)] ## TRU VAI LOC HOT TOX BAR SPI GOU BRO PER BOU PSO ROT CAR BCO PCH GRE GAR BBO ABL ## 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## ANG ## 2 # Extract their indicator values doubs.mrt.indval$indcls[which(doubs.mrt.indval$pval &lt;= 0.05)] ## TRU VAI LOC HOT TOX BAR SPI GOU ## 0.8674301 0.7758443 0.7042392 0.8571429 0.6185282 0.6363569 0.7347359 0.6442950 ## BRO PER BOU PSO ROT CAR BCO PCH ## 0.5533235 0.5449488 0.7857143 0.8070918 0.6352865 0.7307582 0.6428571 0.5000000 ## GRE GAR BBO ABL ANG ## 0.8571429 0.7726181 0.7142857 1.0000000 0.7857143 TRU has the highest indicator value (0.867) overall, and is an indicator species for the first (alt &gt;= 361.5) leaf of the tree. 9.3 Challenge 4 Create a multivariate regression tree for the mite data. * Select the smallest tree within 1 SE of the CVRE. * What is the proportion of variance (R2) explained by this tree? * How many leaves does it have? * What are the top 3 discriminant species? Remember to load the mite data: data(&quot;mite&quot;) data(&quot;mite.env&quot;) Recall some useful functions: `?`(mvpart() # hint: pay attention to the &#39;xv&#39; argument! ) summary() 9.3.1 Challenge 4: Solution Step 1: Create the multivariate regression tree. mite.mrt &lt;- mvpart(as.matrix(mite.spe.hel) ~ ., data = mite.env, xv = &quot;1se&quot;, # choose smallest tree within 1 SE xval = nrow(mite.spe.hel), xvmult = 100, which = 4, legend = FALSE, margin = 0.01, cp = 0, prn = FALSE) ## X-Val rep : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 ## Minimum tree sizes ## tabmins ## 2 3 4 5 6 7 8 9 12 15 ## 1 11 3 9 9 24 33 8 1 1 What is the proportion of variance (\\(R^2\\)) explained by this tree? * \\(1 - 0.748 = 0.252\\), so the tree explains 25.2% of the variance in the species matrix. How many leaves does it have? * 2 leaves Step 2: Identify the indicator species. Which species are significant indicator species for each grouping of sites? # Calculate indicator values (indval) for each species mite.mrt.indval &lt;- indval(mite.spe.hel, mite.mrt$where) # Extract the significant indicator species (and which node # they represent) mite.mrt.indval$maxcls[which(mite.mrt.indval$pval &lt;= 0.05)] ## PHTH RARD SSTR Protopl MEGR MPRO TVIE HMIN ## 2 2 2 2 2 2 1 2 ## HMIN2 NPRA TVEL ONOV SUCT LCIL Oribatl1 Ceratoz1 ## 2 2 2 2 2 1 2 1 ## PWIL Galumna1 Stgncrs2 Trhypch1 NCOR SLAT FSET Lepidzts ## 2 2 2 1 1 2 2 2 ## Miniglmn LRUG Ceratoz3 Trimalc2 ## 2 1 1 1 # Extract their indicator values mite.mrt.indval$indcls[which(mite.mrt.indval$pval &lt;= 0.05)] ## PHTH RARD SSTR Protopl MEGR MPRO TVIE HMIN ## 0.5317919 0.5584677 0.2256592 0.2517509 0.5769554 0.1567789 0.3793303 0.6421174 ## HMIN2 NPRA TVEL ONOV SUCT LCIL Oribatl1 Ceratoz1 ## 0.6193076 0.4620892 0.7412296 0.6312483 0.6087557 0.7152107 0.5978167 0.4744997 ## PWIL Galumna1 Stgncrs2 Trhypch1 NCOR SLAT FSET Lepidzts ## 0.3779883 0.5974145 0.3897917 0.4545803 0.4539642 0.2249109 0.6361272 0.2108305 ## Miniglmn LRUG Ceratoz3 Trimalc2 ## 0.1880194 0.6683300 0.3962540 0.4358974 References "],["linear-discriminant-analysis.html", "Chapter 10 Linear discriminant analysis 10.1 LDA in R: Doubs River fish dataset 10.2 Challenge 5", " Chapter 10 Linear discriminant analysis Linear discriminant analysis (LDA) is a constrained (canonical) technique that divides a response mtrix into groups according to a factor by finding combination of the variables that give best possible separation between groups. The grouping is done by maximizing the among-group dispersion versus the within-group dispersion. This allows you to determine how well your independent set of variables explains an a priori grouping, which may have been obtained from a previous clustering analysis (see Workshop 9) or from a hypothesis (e.g. grouping is based on sites at different latitudes or different treatments). An LDA computes discriminant functions from standardized descriptors. These coefficients quantify the relative contributions of the (standardized) explanatory variables to the discrimination of objects. Identification functions can be computed from the original (not standardized) descriptors to classify new data into pre-determined groups. There are several useful applications of this capacity to predict groupings, such as assessing which population a fish should be classified in based on morphology or classifying whether a new paper is a freshwater, marine or terrestrial study based on the abstract of papers in those pre-determined biomes. 10.1 LDA in R: Doubs River fish dataset We will continue to work with the Doubs River fish dataset to demonstrate how to implement a linear discriminant analysis in R. 10.1.1 Making a priori groupings First, we want to make an a priori classification that is independent from the environmental data set. We know that environmental variables generally change with latitude. This might lead us to ask the following question: If we classify our Doubs River sites according to latitude, how well do environmental variables explain these groupings? To answer this question, we can make a priori groups by simply dividing the range of latitudes equally into three groups, and assigning each site to a group depending on where they fall along the divided range. Let us begin by loading spatial coordinates for the Doubs River sites. # load spatial data for Doubs sites spa &lt;- read.csv(&quot;data/doubsspa.csv&quot;, row.names = 1) spa$site &lt;- 1:nrow(spa) # add site numbers spa &lt;- spa[-8, ] # remove site 8 We can then assign sites into three groups based on their latitude. # group sites based on latitude spa$group &lt;- NA # create &#39;group&#39; column spa$group[which(spa$y &lt; 82)] &lt;- 1 spa$group[which(spa$y &gt; 82 &amp; spa$y &lt; 156)] &lt;- 2 spa$group[which(spa$y &gt; 156)] &lt;- 3 Let us quickly plot the latitude groupings to see if they make sense. ggplot(data = spa) + geom_point(aes(x = x, y = y, col = as.factor(group)), size = 4) + labs(color = &quot;Groups&quot;, x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + scale_color_manual(values = c(&quot;#3b5896&quot;, &quot;#e3548c&quot;, &quot;#ffa600&quot;)) + theme_classic() + # formatting the plot to make it pretty theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16), legend.title = element_text(size = 20), legend.text = element_text(size = 18)) Now that we have our a priori groups, we would usually need to ensure the data meets a condition necessary for the application of LDA: the within-group covariance matrices of the explanatory variables must be homogeneous. To do this, we would use the betadisper() function in the vegan package to check the multivariate homogeneity of within-group variances before proceeding, as seen in Borcard, Gillet, and Legendre (2011). For the purposes of this workshop, we will move straight to doing the LDA. 10.1.2 Running the LDA # run the LDA grouping sites into latitude groups based on # env data LDA &lt;- lda(env, spa$group) Our sites have now been reorganised into groups that are as distinct as possible, based on the environmental variables. We can plot the reorganised sites to visualise the results of the LDA. # predict the groupings lda.plotdf &lt;- data.frame(group = spa$group, lda = predict(LDA)$x) # Plot the newly reorganised sites according to the LDA ggplot(lda.plotdf) + geom_point(aes(x = lda.LD1, y = lda.LD2, col = factor(group)), size = 4) + labs(color = &quot;Groups&quot;) + scale_color_manual(values = c(&quot;#3b5896&quot;, &quot;#e3548c&quot;, &quot;#ffa600&quot;)) + theme_classic() + # formatting the plot to make it pretty theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16), legend.title = element_text(size = 20), legend.text = element_text(size = 18)) 10.1.3 Evaluating grouping accuracy Once we run the LDA, we can use the result object to determine: How sites are grouped based on the the environmental data, according to the LDA; The posterior probabilities of that the sites to belong to the groups; The percentage of correct classification based on our latitudinal grouping. # Classification of the objects based on the LDA spe.class &lt;- predict(LDA)$class # Posterior probabilities that the objects belong to those # groups spe.post &lt;- predict(LDA)$posterior # Table of prior vs. predicted classifications (spe.table &lt;- table(spa$group, spe.class)) ## spe.class ## 1 2 3 ## 1 7 0 0 ## 2 0 6 0 ## 3 0 0 16 # Proportion of corrected classification diag(prop.table(spe.table, 1)) ## 1 2 3 ## 1 1 1 All sites were correctly classified (Proportion of corrected classification = 1) into the latitude groups based on environmental variables. 10.1.4 Predictions We can now use this relationship to classify new sites into latitude groups, based on the relationship we have established between our latitudinal grouping and environmental factors using the LDA. Let us predict the groupings of five new sites in the dummy dataset classifyme.csv using the LDA object we computed above. To do this, we will use the predict() function. # Load the new site data classify.me &lt;- read.csv(&quot;data/classifyme.csv&quot;, header = TRUE) # remove das classify.me &lt;- subset(classify.me, select = -das) # Predict grouping of new sites predict.group &lt;- predict(LDA, newdata = classify.me) # View site classification predict.group$class ## [1] 1 1 1 3 3 ## Levels: 1 2 3 Our new sites, in order, have been classified in groups 1, 1, 1, 3 and 3 respectively. 10.2 Challenge 5 Create four equally-spaced latitude groups in the mite.xy dataset. Then, run an LDA to classify mite sites into latitude groupings based on environmental variables (SubsDens and WatrCont). What proportion of sites was correctly classified in group1? in group2? To begin, load the mite.xy data: data(mite.xy) Recall some useful functions: lda() predict() table() diag() Step 1: Make four equally spaced latitude groups. # assign numbers to sites mite.xy$site &lt;- 1:nrow(mite.xy) # figure out the spacing to make 4 equally spaced latitude # groups (max(mite.xy[, 2]) - min(mite.xy[, 2]))/4 ## [1] 2.4 # use this to group sites into 4 latitude groups mite.xy$group &lt;- NA # create &#39;group&#39; column mite.xy$group[which(mite.xy$y &lt; 2.5)] &lt;- 1 mite.xy$group[which(mite.xy$y &gt;= 2.5 &amp; mite.xy$y &lt; 4.9)] &lt;- 2 mite.xy$group[which(mite.xy$y &gt;= 4.9 &amp; mite.xy$y &lt; 7.3)] &lt;- 3 mite.xy$group[which(mite.xy$y &gt;= 7.3)] &lt;- 4 Step 2: Run the LDA. LDA.mite &lt;- lda(mite.env[, 1:2], mite.xy$group) Step 3: Check whether the groupings are correct. # group sites based on the LDA mite.class &lt;- predict(LDA.mite)$class # get the table of prior versus predicted classifications (mite.table &lt;- table(mite.xy$group, mite.class)) ## mite.class ## 1 2 3 4 ## 1 9 4 2 0 ## 2 2 11 4 0 ## 3 1 2 14 2 ## 4 0 0 3 16 We can answer the challenge question with this part: # proportion of correct classification diag(prop.table(mite.table, 1)) ## 1 2 3 4 ## 0.6000000 0.6470588 0.7368421 0.8421053 So, what proportion of sites was correctly classified in group1? in group2? 60% were correctly classified into group1, and 64.7% were classified into group2. References "],["summary.html", "Chapter 11 Summary", " Chapter 11 Summary This workshop covered a handful of constrained analyses, which allow us to test hypotheses about the drivers of patterns in a response matrix, such as a matrix describing the abundance of species sampled across many sites. We can use RDA, partial RDA, and variation partitioning to quantify the importance of different variables (or, groups of variables) on a response matrix. In many cases, this response matrix was a community composition matrix of sites x species, but these techniques are not limited to community ecology. We also saw two ways of testing hypotheses about site groupings. We can use multivariate regression trees (MRT) to determine which explanatory variables distinguish groups of sites, and describe how our response matrix is organised into these distinct groups. If we already have an a priori grouping of sites, we can use linear discriminant analysis (LDA) to verify whether this grouping aligns with environmental data, and predict the grouping of new sites. "],["additional-resources.html", "Chapter 12 Additional resources", " Chapter 12 Additional resources Unfortunately, we could only cover a subset of constrained ordinations in this workshop. However, there are many other options! These include, but are not limited to, the following techniques: Constrained Correspondence Analysis (CCA) is a canonical ordination method similar to RDA that preserve Chi-square distances among object (instead of Euclidean distances in RDA). This method is well suited for the analysis of large ecological gradients. Canonical Correlation Analysis (CCorA) differs from RDA given that the two matrices are considered symmetric, while in RDA the Y matrix is dependent on the X matrix. The main use of this technique is to test the significance of the correlation between two multidimensional data sets, then explore the structure of the data by computing the correlations (which are the square roots of the CCorA eigenvalues) that can be found between linear functions of two groups of descriptors. Coinertia Analysis (CoIA) is a symmetric canonical ordination method that is appropriate to compare pairs of data sets that play equivalent roles in the analysis. The method finds a common space onto which the objects and variables of these data sets can be projected and compared. Compared to CCorA, co-inertia analysis imposes no constraint regarding the number of variables in the two sets, so that it can be used to compare ecological communities even when they are species-rich. Co-inertia analysis is not well-suited, however, to analyse pairs of data sets that contain the same variables, because the analysis does not establish one-to-one correspondences between variables in the two data sets; the method does not ‘know’ that the first variable is the same in the first and the second data sets, and likewise for the other variables. Multiple factor analysis (MFA) can be used to compare several data sets describing the same objects. MFA consists in projecting objects and variables of two or more data sets on a global PCA, computed from all data sets, in which the sets receive equal weights. `?`(cca # (constrained correspondence analysis) ) `?`(CCorA # Canonical Correlation Analysis ) help(coinertia, package = ade4) # Coinertia Analysis help(mfa, package = ade4) # Multiple Factorial Analysis # Spatial analysis can be performed using the adespatial # package. Spatial eigenfunctions can be calculated with # dbmem(), and these are functionally the same as PCNM # which we saw in the mite.pcnm dataset from vegan. # https://cran.r-project.org/web/packages/adespatial/index.html Our list of references includes many useful articles and books to pursue constrained ordinations in more depth. We specifically recommend P. Legendre and Legendre (2012) for a thorough view of these techniques, their computation, and potential applications. We also recommend Borcard, Gillet, and Legendre (2011) to learn more about how these techniques can be implemented in R. References "],["references.html", "Chapter 13 References", " Chapter 13 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
