<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Multivariate regression tree | Workshop 10: Advanced Multivariate Analyses in R</title>
  <meta name="description" content="Advanced Multivariate Analyses in R" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Multivariate regression tree | Workshop 10: Advanced Multivariate Analyses in R" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://github.com/qcbsRworkshops/" />
  <meta property="og:image" content="https://github.com/qcbsRworkshops//assets/images/logo/csbq_logo_accueil.png" />
  <meta property="og:description" content="Advanced Multivariate Analyses in R" />
  <meta name="github-repo" content="qcbsRworkshops/workshop10" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Multivariate regression tree | Workshop 10: Advanced Multivariate Analyses in R" />
  
  <meta name="twitter:description" content="Advanced Multivariate Analyses in R" />
  <meta name="twitter:image" content="https://github.com/qcbsRworkshops//assets/images/logo/csbq_logo_accueil.png" />

<meta name="author" content="Developed and maintained by the contributors of the QCBS R Workshop Series" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="assets/images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="variation-partitioning.html"/>
<link rel="next" href="linear-discriminant-analysis.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="assets/qcbs-style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="assets/images/csbq_logo_gray_accueil.png"></a></li>
<link rel="icon" type="image/png" href="assets/images/favicon.ico"/>

<li class="divider"></li>
<li class="part"><span><b>QCBS R Workshop Series</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#code-of-conduct"><i class="fa fa-check"></i><b>0.1</b> Code of conduct</a><ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#expected-behaviour"><i class="fa fa-check"></i><b>0.1.1</b> Expected behaviour</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#unacceptable-behaviour"><i class="fa fa-check"></i><b>0.1.2</b> Unacceptable behaviour</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i><b>0.2</b> Contributors</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#contributing"><i class="fa fa-check"></i><b>0.3</b> Contributing</a></li>
</ul></li>
<li class="part"><span><b>Advanced Multivariate Analyses in <code>R</code></b></span></li>
<li class="chapter" data-level="1" data-path="learning-objectives.html"><a href="learning-objectives.html"><i class="fa fa-check"></i><b>1</b> Learning objectives</a></li>
<li class="chapter" data-level="2" data-path="preparing-for-the-workshop.html"><a href="preparing-for-the-workshop.html"><i class="fa fa-check"></i><b>2</b> Preparing for the workshop</a></li>
<li class="chapter" data-level="3" data-path="why-advanced-multivariate-methods.html"><a href="why-advanced-multivariate-methods.html"><i class="fa fa-check"></i><b>3</b> Why “advanced multivariate methods”?</a></li>
<li class="chapter" data-level="4" data-path="exploration.html"><a href="exploration.html"><i class="fa fa-check"></i><b>4</b> Exploring the Doubs River dataset</a><ul>
<li class="chapter" data-level="4.1" data-path="exploration.html"><a href="exploration.html#exploring-the-fish-community-dataset"><i class="fa fa-check"></i><b>4.1</b> Exploring the fish community dataset</a></li>
<li class="chapter" data-level="4.2" data-path="exploration.html"><a href="exploration.html#exploring-the-environmental-dataset"><i class="fa fa-check"></i><b>4.2</b> Exploring the environmental dataset</a><ul>
<li class="chapter" data-level="4.2.1" data-path="exploration.html"><a href="exploration.html#collinearity"><i class="fa fa-check"></i><b>4.2.1</b> Collinearity</a></li>
<li class="chapter" data-level="4.2.2" data-path="exploration.html"><a href="exploration.html#standardizing-the-environmental-variables"><i class="fa fa-check"></i><b>4.2.2</b> Standardizing the environmental variables</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Constrained ordinations</b></span></li>
<li class="chapter" data-level="5" data-path="what-are-constrained-ordinations.html"><a href="what-are-constrained-ordinations.html"><i class="fa fa-check"></i><b>5</b> What are “constrained” ordinations?</a></li>
<li class="chapter" data-level="6" data-path="redundancy-analysis.html"><a href="redundancy-analysis.html"><i class="fa fa-check"></i><b>6</b> Redundancy analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="redundancy-analysis.html"><a href="redundancy-analysis.html#how-the-rda-works"><i class="fa fa-check"></i><b>6.1</b> How the RDA works</a></li>
<li class="chapter" data-level="6.2" data-path="redundancy-analysis.html"><a href="redundancy-analysis.html#running-an-rda-in-r"><i class="fa fa-check"></i><b>6.2</b> Running an RDA in R</a><ul>
<li class="chapter" data-level="6.2.1" data-path="redundancy-analysis.html"><a href="redundancy-analysis.html#selecting-variables"><i class="fa fa-check"></i><b>6.2.1</b> Selecting variables</a></li>
<li class="chapter" data-level="6.2.2" data-path="redundancy-analysis.html"><a href="redundancy-analysis.html#significance-testing"><i class="fa fa-check"></i><b>6.2.2</b> Significance testing</a></li>
<li class="chapter" data-level="6.2.3" data-path="redundancy-analysis.html"><a href="redundancy-analysis.html#rda-plot"><i class="fa fa-check"></i><b>6.2.3</b> RDA plot</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="redundancy-analysis.html"><a href="redundancy-analysis.html#challenge-1"><i class="fa fa-check"></i><b>6.3</b> Challenge 1</a><ul>
<li class="chapter" data-level="6.3.1" data-path="redundancy-analysis.html"><a href="redundancy-analysis.html#challenge-1-solution"><i class="fa fa-check"></i><b>6.3.1</b> Challenge 1: Solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="partial-redundancy-analysis.html"><a href="partial-redundancy-analysis.html"><i class="fa fa-check"></i><b>7</b> Partial Redundancy Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="partial-redundancy-analysis.html"><a href="partial-redundancy-analysis.html#example-partial-rda-on-doubs-river-data"><i class="fa fa-check"></i><b>7.1</b> Example: Partial RDA on Doubs River data</a><ul>
<li class="chapter" data-level="7.1.1" data-path="partial-redundancy-analysis.html"><a href="partial-redundancy-analysis.html#interpreting-partial-rda-output-in-r"><i class="fa fa-check"></i><b>7.1.1</b> Interpreting partial RDA output in R</a></li>
<li class="chapter" data-level="7.1.2" data-path="partial-redundancy-analysis.html"><a href="partial-redundancy-analysis.html#significance-testing-1"><i class="fa fa-check"></i><b>7.1.2</b> Significance testing</a></li>
<li class="chapter" data-level="7.1.3" data-path="partial-redundancy-analysis.html"><a href="partial-redundancy-analysis.html#partial-rda-plot"><i class="fa fa-check"></i><b>7.1.3</b> Partial RDA plot</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="partial-redundancy-analysis.html"><a href="partial-redundancy-analysis.html#challenge-2"><i class="fa fa-check"></i><b>7.2</b> Challenge 2</a><ul>
<li class="chapter" data-level="7.2.1" data-path="partial-redundancy-analysis.html"><a href="partial-redundancy-analysis.html#challenge-2-solution"><i class="fa fa-check"></i><b>7.2.1</b> Challenge 2: Solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="variation-partitioning.html"><a href="variation-partitioning.html"><i class="fa fa-check"></i><b>8</b> Variation partitioning</a><ul>
<li class="chapter" data-level="8.1" data-path="variation-partitioning.html"><a href="variation-partitioning.html#variation-partitioning-in-r"><i class="fa fa-check"></i><b>8.1</b> Variation partitioning in R</a></li>
<li class="chapter" data-level="8.2" data-path="variation-partitioning.html"><a href="variation-partitioning.html#significance-testing-2"><i class="fa fa-check"></i><b>8.2</b> Significance testing</a></li>
<li class="chapter" data-level="8.3" data-path="variation-partitioning.html"><a href="variation-partitioning.html#challenge-3"><i class="fa fa-check"></i><b>8.3</b> Challenge 3</a><ul>
<li class="chapter" data-level="8.3.1" data-path="variation-partitioning.html"><a href="variation-partitioning.html#challenge-3-solution"><i class="fa fa-check"></i><b>8.3.1</b> Challenge 3: Solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html"><i class="fa fa-check"></i><b>9</b> Multivariate regression tree</a><ul>
<li class="chapter" data-level="9.1" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#computing-the-mrt"><i class="fa fa-check"></i><b>9.1</b> Computing the MRT</a><ul>
<li class="chapter" data-level="9.1.1" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#building-the-tree-constrained-partitioning-of-the-data"><i class="fa fa-check"></i><b>9.1.1</b> Building the tree: Constrained partitioning of the data</a></li>
<li class="chapter" data-level="9.1.2" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#selecting-the-tree-cross-validation-and-pruning"><i class="fa fa-check"></i><b>9.1.2</b> Selecting the tree: Cross-validation and pruning</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#mrt-in-r"><i class="fa fa-check"></i><b>9.2</b> MRT in R</a><ul>
<li class="chapter" data-level="9.2.1" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#mrt-selection-process"><i class="fa fa-check"></i><b>9.2.1</b> MRT selection process</a></li>
<li class="chapter" data-level="9.2.2" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#interpreting-mrt-output"><i class="fa fa-check"></i><b>9.2.2</b> Interpreting MRT output</a></li>
<li class="chapter" data-level="9.2.3" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#indicator-species"><i class="fa fa-check"></i><b>9.2.3</b> Indicator species</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#challenge-4"><i class="fa fa-check"></i><b>9.3</b> Challenge 4</a><ul>
<li class="chapter" data-level="9.3.1" data-path="multivariate-regression-tree.html"><a href="multivariate-regression-tree.html#challenge-4-solution"><i class="fa fa-check"></i><b>9.3.1</b> Challenge 4: Solution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html"><i class="fa fa-check"></i><b>10</b> Linear discriminant analysis</a><ul>
<li class="chapter" data-level="10.1" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html#lda-in-r-doubs-river-fish-dataset"><i class="fa fa-check"></i><b>10.1</b> LDA in R: Doubs River fish dataset</a><ul>
<li class="chapter" data-level="10.1.1" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html#making-a-priori-groupings"><i class="fa fa-check"></i><b>10.1.1</b> Making <em>a priori</em> groupings</a></li>
<li class="chapter" data-level="10.1.2" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html#running-the-lda"><i class="fa fa-check"></i><b>10.1.2</b> Running the LDA</a></li>
<li class="chapter" data-level="10.1.3" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html#evaluating-grouping-accuracy"><i class="fa fa-check"></i><b>10.1.3</b> Evaluating grouping accuracy</a></li>
<li class="chapter" data-level="10.1.4" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html#predictions"><i class="fa fa-check"></i><b>10.1.4</b> Predictions</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="linear-discriminant-analysis.html"><a href="linear-discriminant-analysis.html#challenge-5"><i class="fa fa-check"></i><b>10.2</b> Challenge 5</a></li>
</ul></li>
<li class="part"><span><b>Final considerations</b></span></li>
<li class="chapter" data-level="11" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>11</b> Summary</a></li>
<li class="chapter" data-level="12" data-path="additional-resources.html"><a href="additional-resources.html"><i class="fa fa-check"></i><b>12</b> Additional resources</a></li>
<li class="chapter" data-level="13" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>13</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/qcbsRworkshops" target="blank">QCBS R Workshop Series</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Workshop 10: Advanced Multivariate Analyses in <code>R</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!------------------------ Hero Image Container --------------------------> 

<head>
  <meta name="viewport" content="width=device-width,minimum-scale=1.0,maximum-scale=10.0,initial-scale=1.0">
  <script src="https://kit.fontawesome.com/6a26f47516.js"></script>
  <script src="assets/qcbs-hideOutput.js"></script>
  <link href="assets/qcbs-style.css" rel="stylesheet">
</head>



<div class="hero-image-container"> 
  <img class= "hero-image" src="assets/images/jean-philippe-delberghe-75xPHEQBmvA-unsplash_hero_image.jpg">
</div>
<div id="multivariate-regression-tree" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Multivariate regression tree</h1>
<p>Multivariate regression tree (MRT) is a <strong>hierarchical constrained clustering technique</strong>. Introduced by <span class="citation">De’ath (<a href="#ref-de2002multivariate" role="doc-biblioref">2002</a>)</span>, the MRT splits a response matrix (<span class="math inline">\(Y\)</span>) into clusters based on thresholds of explanatory variables (<span class="math inline">\(X\)</span>). Like RDA, MRT is a regression technique. While the former explains the global structure of relationships through a linear model, the latter produces a tree model to highlight local structures and interactions among variables.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-71"></span>
<img src="images/MRT.png" alt="The basic structure of a multivariate regression tree (MRT)." width="70%" />
<p class="caption">
Figure 9.1: The basic structure of a multivariate regression tree (MRT).
</p>
</div>
<p>MRT has many convenient characteristics:</p>
<ul>
<li>It does not assume a linear relationship between Y and X matrices;</li>
<li>The results are easy to visualize and interpret (it’s a tree!);</li>
<li>It clearly identifies importance of explanatory variables;</li>
<li>It is robust to missing values;</li>
<li>It is robust to collinearity among the explanatory variables;</li>
<li>It can handle raw explanatory variables, meaning there is no need to standardize.</li>
</ul>
<div class="explanation">
<p>A quick note on MRT terminology:</p>
<ul>
<li><strong>Branch</strong>: each group formed by a split;<br />
</li>
<li><strong>Node</strong>: splitting point (threshold value of an explanatory variable);<br />
</li>
<li><strong>Leaf</strong>: terminal group of sites.<br />
</li>
</ul>
</div>
<div id="computing-the-mrt" class="section level2">
<h2><span class="header-section-number">9.1</span> Computing the MRT</h2>
<p>The MRT splits the data into clusters of samples similar in their species composition based on environmental value thresholds. It involves two procedures running at the same time: <strong>1)</strong> the computation of the constrained partitioning of the data, and <strong>2)</strong> the calculation of the relative error of the successive partitioning levels by multiple cross-validations. This cross-validation is, in essence, aiming to identify best predictive tree. The “<em>best</em>” tree varies depending on your study goals. Usually you want a tree that is parsimonious, but still has an informative number of groups. This is, of course, a subjective decision to make according to the question you are trying to answer.</p>
<div id="building-the-tree-constrained-partitioning-of-the-data" class="section level3">
<h3><span class="header-section-number">9.1.1</span> Building the tree: Constrained partitioning of the data</h3>
<p>First, the method computes all possible partitions of the sites into two groups. For each quantitative explanatory variable, the sites will be sorted in the ascending values of the variables. For categorical variables, the sites will be aggregated by levels to test all combinations of levels. The method will split the data after the first object, the second object and so on, and compute the sum of within-group sum of squared distances to the group mean (within-group SS) for the response data. The method will retain the partition into two groups minimizing the within-group SS and the threshold value/level of the explanatory variable. These steps will be repeated within the two subgroups formed previously, until all objects form their own group. In other words, this process ends when each leaf of the tree contains one object.</p>
</div>
<div id="selecting-the-tree-cross-validation-and-pruning" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Selecting the tree: Cross-validation and pruning</h3>
<p>The next step is to perform a cross-validation and identify the best predictive tree. The cross-validation procedure consists in using a subset of the objects to construct the tree, and to allocate the remaining objects to the groups. In a <em>good</em> predictive tree, objects are assigned to the appropriate groups. The <strong>cross-validated relative error (CVRE)</strong> is the measure of the predictive error. Without cross-validation, one would retain the number of partitions minimizing the variance not explained by the tree (i.e. the relative error: the sum of the within-group SS over all leaves divided by the overall SS of the data). This is the solution which maximizes the <span class="math inline">\(R^2\)</span>, so to speak.</p>
</div>
</div>
<div id="mrt-in-r" class="section level2">
<h2><span class="header-section-number">9.2</span> MRT in R</h2>
<p>The function <code>mvpart()</code> from the package <code>mvpart</code> computes both the partition and the cross-validation steps required to build a multivariate regression tree.</p>
<p>We will demonstrate the process of building a multivariate regression tree on the Doubs River data.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="multivariate-regression-tree.html#cb97-1"></a><span class="co"># First, remove the &quot;distance from source&quot; variable</span></span>
<span id="cb97-2"><a href="multivariate-regression-tree.html#cb97-2"></a>env &lt;-<span class="st"> </span><span class="kw">subset</span>(env, <span class="dt">select =</span> <span class="op">-</span>das)</span>
<span id="cb97-3"><a href="multivariate-regression-tree.html#cb97-3"></a></span>
<span id="cb97-4"><a href="multivariate-regression-tree.html#cb97-4"></a><span class="co"># Create multivariate regression tree</span></span>
<span id="cb97-5"><a href="multivariate-regression-tree.html#cb97-5"></a>doubs.mrt &lt;-<span class="st"> </span><span class="kw">mvpart</span>(<span class="kw">as.matrix</span>(spe.hel) <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> env,</span>
<span id="cb97-6"><a href="multivariate-regression-tree.html#cb97-6"></a>                    <span class="dt">xv =</span> <span class="st">&quot;pick&quot;</span>, <span class="co"># interactively select best tree</span></span>
<span id="cb97-7"><a href="multivariate-regression-tree.html#cb97-7"></a>                    <span class="dt">xval =</span> <span class="kw">nrow</span>(spe.hel), <span class="co"># number of cross-validations</span></span>
<span id="cb97-8"><a href="multivariate-regression-tree.html#cb97-8"></a>                    <span class="dt">xvmult =</span> <span class="dv">100</span>, <span class="co"># number of multiple cross-validations</span></span>
<span id="cb97-9"><a href="multivariate-regression-tree.html#cb97-9"></a>                    <span class="dt">which =</span> <span class="dv">4</span>, <span class="co"># plot both node labels</span></span>
<span id="cb97-10"><a href="multivariate-regression-tree.html#cb97-10"></a>                    <span class="dt">legend =</span> <span class="ot">FALSE</span>, <span class="dt">margin =</span> <span class="fl">0.01</span>, <span class="dt">cp =</span> <span class="dv">0</span>)</span></code></pre></div>
<pre><code>## X-Val rep : 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100
## Minimum tree sizes
## tabmins
##  2  3  5  6  7  8  9 10 
##  9  1  1  8 16 24  6 35</code></pre>
<p><img src="book-en_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<p>At this point, you will need to select the tree with an <em>appropriate</em> number of groups, depending on the aim of your study. In other words, you must prune the tree by picking the best-fit tree. A fully resolved tree is not the desirable outcome; instead, one is usually interested in a tree including only informative partitions/groups. In such cases, it is possible to have an <em>a priori</em> idea of the number of potential groups to be retained. You can make this choice interactively, with the argument <code>xv = "pick"</code>.</p>
<p>The resulting figure shows the relative error RE (in green) and the cross-validated relative error CVRE (in blue) of trees of increasing size. The red dot indicates the solution with the smallest CVRE, and the orange dot shows the smallest tree within one standard error of CVRE. It has been suggested that instead of choosing the solution minimizing CVRE, it would be more parsimonious to opt for the smallest tree for which the CVRE is within one standard error of the tree with the lowest CVRE <span class="citation">Breiman et al. (<a href="#ref-breiman1984classification" role="doc-biblioref">1984</a>)</span>. The green bars at the top indicate the number of times each size was chosen during the cross-validation process. This graph is interactive, which means you will have to click on the blue point corresponding your choice of tree size. In summary:</p>
<ul>
<li>Green points: Relative error</li>
<li>Blue points: Cross-validated relative error (CVRE)</li>
<li>Red dot: Which tree has the smallest CVRE</li>
<li>Orange dot: Smallest tree within one standard error of the CVRE</li>
<li>Lime green bars: number of times each tree size was chosen</li>
</ul>
<p>We don’t have an <em>a priori</em> expectation about how to partition this data, so we’ll select the smallest tree within 1 standard error of the overall best-fit tree (i.e. the orange dot). We can select this tree using the <code>xv = "1se"</code> argument.</p>
<p>The statistics at the bottom of the figure are: the residual error, the cross-validated error, and the standard error. This tree has only two leaves separated by one node. Each leaf is characterized by a small barplot showing the abundances of the species included in the group, the number of sites in the group, and the group’s relative error. From this figure, we can report the following statistics:
* The species matrix is partitioned according to an altitude threshold (361.5 m)
* Residual error = <strong>0.563</strong>, which means the model’s <span class="math inline">\(R^2\)</span> is <strong>43.7%</strong> (<span class="math inline">\(1 - 0.563 = 0.437\)</span>)</p>
<div id="mrt-selection-process" class="section level3">
<h3><span class="header-section-number">9.2.1</span> MRT selection process</h3>
<p>We can also compare solutions, to help us chose the best tree. For example, let’s take a look at a 10-group solution!</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="multivariate-regression-tree.html#cb99-1"></a><span class="co"># Trying 10 groups</span></span>
<span id="cb99-2"><a href="multivariate-regression-tree.html#cb99-2"></a><span class="kw">mvpart</span>(<span class="kw">as.matrix</span>(spe.hel) <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> env,</span>
<span id="cb99-3"><a href="multivariate-regression-tree.html#cb99-3"></a>        <span class="dt">xv =</span> <span class="st">&quot;none&quot;</span>, <span class="co"># no cross-validation</span></span>
<span id="cb99-4"><a href="multivariate-regression-tree.html#cb99-4"></a>        <span class="dt">size =</span> <span class="dv">10</span>, <span class="co"># set tree size</span></span>
<span id="cb99-5"><a href="multivariate-regression-tree.html#cb99-5"></a>        <span class="dt">which =</span> <span class="dv">4</span>,</span>
<span id="cb99-6"><a href="multivariate-regression-tree.html#cb99-6"></a>        <span class="dt">legend =</span> <span class="ot">FALSE</span>, <span class="dt">margin =</span> <span class="fl">0.01</span>, <span class="dt">cp =</span> <span class="dv">0</span>, <span class="dt">prn =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p><img src="book-en_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<p>This tree is much harder to interpret, because there are so many groups! Although this version of the tree offers higher explanatory power, its <em>predictive</em> power (CV Error = 0.671) is basically the same as the previous two-group solution (CV Error = 0.673). This suggests that we may want to try a tree with a few more groupings than the two-group solution, while staying lower than 10 groups.</p>
<p>Let’s look at a solution with fewer (4) groups!</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="multivariate-regression-tree.html#cb100-1"></a><span class="co"># Trying fewer groups</span></span>
<span id="cb100-2"><a href="multivariate-regression-tree.html#cb100-2"></a><span class="kw">mvpart</span>(<span class="kw">as.matrix</span>(spe.hel) <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> env,</span>
<span id="cb100-3"><a href="multivariate-regression-tree.html#cb100-3"></a>        <span class="dt">xv =</span> <span class="st">&quot;none&quot;</span>, <span class="co"># no cross-validation</span></span>
<span id="cb100-4"><a href="multivariate-regression-tree.html#cb100-4"></a>        <span class="dt">size =</span> <span class="dv">4</span>, <span class="co"># set tree size</span></span>
<span id="cb100-5"><a href="multivariate-regression-tree.html#cb100-5"></a>        <span class="dt">which =</span> <span class="dv">4</span>,</span>
<span id="cb100-6"><a href="multivariate-regression-tree.html#cb100-6"></a>        <span class="dt">legend =</span> <span class="ot">FALSE</span>, <span class="dt">margin =</span> <span class="fl">0.01</span>, <span class="dt">cp =</span> <span class="dv">0</span>, <span class="dt">prn =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p><img src="book-en_files/figure-html/unnamed-chunk-75-1.png" width="672" /></p>
<p>This tree is much easier to interpret! It also offers higher explanatory power (lower Error) than our original solution, and higher predictive power than both previous solutions (CV Error). We have a winner!</p>
</div>
<div id="interpreting-mrt-output" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Interpreting MRT output</h3>
<p>To find out how much variance is explained by each node in the tree, we need to look at the complexity parameter (CP). The CP at <code>nsplit = 0</code> is the <span class="math inline">\(R^2\)</span> of the entire tree.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="multivariate-regression-tree.html#cb101-1"></a><span class="co"># Checking the complexity parameter</span></span>
<span id="cb101-2"><a href="multivariate-regression-tree.html#cb101-2"></a>doubs.mrt<span class="op">$</span>cptable</span></code></pre></div>
<pre><code>##          CP nsplit rel error    xerror       xstd
## 1 0.4369561      0 1.0000000 1.0772860 0.07494546
## 2 0.1044982      1 0.5630439 0.6778153 0.09551622</code></pre>
<p>The summary then outlines, for each node, the best threshold values to split the data.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="multivariate-regression-tree.html#cb103-1"></a><span class="co"># Checking the tree result summary</span></span>
<span id="cb103-2"><a href="multivariate-regression-tree.html#cb103-2"></a><span class="kw">summary</span>(doubs.mrt)</span></code></pre></div>
<pre><code>## Call:
## mvpart(form = as.matrix(spe.hel) ~ ., data = env, xv = &quot;1se&quot;, 
##     xval = nrow(spe.hel), xvmult = 100, margin = 0.01, which = 4, 
##     legend = FALSE, cp = 0)
##   n= 29 
## 
##          CP nsplit rel error    xerror       xstd
## 1 0.4369561      0 1.0000000 1.0772860 0.07494546
## 2 0.1044982      1 0.5630439 0.6778153 0.09551622
## 
## Node number 1: 29 observations,    complexity param=0.4369561
##   Means=0.07299,0.2472,0.2581,0.2721,0.07133,0.06813,0.06897,0.07664,0.1488,0.2331,0.113,0.07879,0.1724,0.1366,0.1103,0.08216,0.08751,0.07113,0.07312,0.1345,0.06307,0.04423,0.1015,0.1862,0.07713,0.1623,0.07283, Summed MSE=0.4851823 
##   left son=2 (15 obs) right son=3 (14 obs)
##   Primary splits:
##       alt &lt; 361.5 to the right, improve=0.4369561, (0 missing)
##       deb &lt; 23.65 to the left,  improve=0.4369561, (0 missing)
##       amm &lt; 0.06  to the left,  improve=0.3529830, (0 missing)
##       nit &lt; 1.415 to the left,  improve=0.3513335, (0 missing)
##       pen &lt; 1.5   to the right, improve=0.3372429, (0 missing)
## 
## Node number 2: 15 observations
##   Means=0.1208,0.4463,0.4194,0.4035,0.1104,0.09023,0,0.02108,0.1256,0.2164,0.04392,0.01054,0.107,0.09779,0.06853,0,0.01054,0.01617,0.01054,0.09489,0,0,0,0.08629,0,0,0, Summed MSE=0.3194207 
## 
## Node number 3: 14 observations
##   Means=0.02179,0.03391,0.08514,0.1313,0.02945,0.04444,0.1429,0.1362,0.1736,0.2509,0.1871,0.1519,0.2425,0.1781,0.1551,0.1702,0.17,0.13,0.1402,0.177,0.1306,0.09163,0.2103,0.2932,0.1598,0.3362,0.1509, Summed MSE=0.2236343</code></pre>
</div>
<div id="indicator-species" class="section level3">
<h3><span class="header-section-number">9.2.3</span> Indicator species</h3>
<p>You might also be interested in finding out which species are significant indicator species for each grouping of sites.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="multivariate-regression-tree.html#cb105-1"></a><span class="co"># Calculate indicator values (indval) for each species</span></span>
<span id="cb105-2"><a href="multivariate-regression-tree.html#cb105-2"></a>doubs.mrt.indval &lt;-<span class="st"> </span><span class="kw">indval</span>(spe.hel, doubs.mrt<span class="op">$</span>where)</span>
<span id="cb105-3"><a href="multivariate-regression-tree.html#cb105-3"></a></span>
<span id="cb105-4"><a href="multivariate-regression-tree.html#cb105-4"></a><span class="co"># Extract the significant indicator species (and which node</span></span>
<span id="cb105-5"><a href="multivariate-regression-tree.html#cb105-5"></a><span class="co"># they represent)</span></span>
<span id="cb105-6"><a href="multivariate-regression-tree.html#cb105-6"></a>doubs.mrt.indval<span class="op">$</span>maxcls[<span class="kw">which</span>(doubs.mrt.indval<span class="op">$</span>pval <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.05</span>)]</span></code></pre></div>
<pre><code>## CHA TRU VAI LOC HOT TOX BAR SPI GOU BRO PER BOU PSO ROT CAR TAN BCO PCH GRE GAR 
##   1   1   1   1   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2 
## BBO ABL ANG 
##   2   2   2</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="multivariate-regression-tree.html#cb107-1"></a><span class="co"># Extract their indicator values</span></span>
<span id="cb107-2"><a href="multivariate-regression-tree.html#cb107-2"></a>doubs.mrt.indval<span class="op">$</span>indcls[<span class="kw">which</span>(doubs.mrt.indval<span class="op">$</span>pval <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.05</span>)]</span></code></pre></div>
<pre><code>##       CHA       TRU       VAI       LOC       HOT       TOX       BAR       SPI 
## 0.3388662 0.8674301 0.7758443 0.7042392 0.8571429 0.6185282 0.6363569 0.7347359 
##       GOU       BRO       PER       BOU       PSO       ROT       CAR       TAN 
## 0.6442950 0.5533235 0.5449488 0.7857143 0.8070918 0.6352865 0.7307582 0.5115135 
##       BCO       PCH       GRE       GAR       BBO       ABL       ANG 
## 0.6428571 0.5000000 0.8571429 0.7726181 0.7142857 1.0000000 0.7857143</code></pre>
<p>TRU has the highest indicator value (0.867) overall, and is an indicator species for the first (alt &gt;= 361.5) leaf of the tree.</p>
</div>
</div>
<div id="challenge-4" class="section level2">
<h2><span class="header-section-number">9.3</span> Challenge 4</h2>
<p>Create a multivariate regression tree for the mite data.
* Select the smallest tree within 1 SE of the CVRE.
* What is the proportion of variance (R2) explained by this tree?
* How many leaves does it have?
* What are the top 3 discriminant species?</p>
<p>Remember to load the mite data:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="multivariate-regression-tree.html#cb109-1"></a><span class="kw">data</span>(<span class="st">&quot;mite&quot;</span>)</span>
<span id="cb109-2"><a href="multivariate-regression-tree.html#cb109-2"></a><span class="kw">data</span>(<span class="st">&quot;mite.env&quot;</span>)</span></code></pre></div>
<p>Recall some useful functions:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="multivariate-regression-tree.html#cb110-1"></a><span class="st">`</span><span class="dt">?</span><span class="st">`</span>(<span class="kw">mvpart</span>()  <span class="co"># hint: pay attention to the &#39;xv&#39; argument!</span></span>
<span id="cb110-2"><a href="multivariate-regression-tree.html#cb110-2"></a>)</span>
<span id="cb110-3"><a href="multivariate-regression-tree.html#cb110-3"></a><span class="kw">summary</span>()</span></code></pre></div>
<div id="challenge-4-solution" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Challenge 4: Solution</h3>
<p><strong>Step 1:</strong> Create the multivariate regression tree.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="multivariate-regression-tree.html#cb111-1"></a>mite.mrt &lt;-<span class="st"> </span><span class="kw">mvpart</span>(<span class="kw">as.matrix</span>(mite.spe.hel) <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mite.env,</span>
<span id="cb111-2"><a href="multivariate-regression-tree.html#cb111-2"></a>                   <span class="dt">xv =</span> <span class="st">&quot;1se&quot;</span>, <span class="co"># choose smallest tree within 1 SE</span></span>
<span id="cb111-3"><a href="multivariate-regression-tree.html#cb111-3"></a>                   <span class="dt">xval =</span> <span class="kw">nrow</span>(mite.spe.hel),</span>
<span id="cb111-4"><a href="multivariate-regression-tree.html#cb111-4"></a>                   <span class="dt">xvmult =</span> <span class="dv">100</span>,</span>
<span id="cb111-5"><a href="multivariate-regression-tree.html#cb111-5"></a>                   <span class="dt">which =</span> <span class="dv">4</span>, <span class="dt">legend =</span> <span class="ot">FALSE</span>, <span class="dt">margin =</span> <span class="fl">0.01</span>, <span class="dt">cp =</span> <span class="dv">0</span>,</span>
<span id="cb111-6"><a href="multivariate-regression-tree.html#cb111-6"></a>                   <span class="dt">prn =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<pre><code>## X-Val rep : 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100
## Minimum tree sizes
## tabmins
##  2  3  4  5  6  7  8  9 12 15 
##  2  7 10  9 11 25 28  5  2  1</code></pre>
<p><img src="book-en_files/figure-html/unnamed-chunk-83-1.png" width="672" /></p>
<p>What is the proportion of variance (<span class="math inline">\(R^2\)</span>) explained by this tree?
* <span class="math inline">\(1 - 0.748 = 0.252\)</span>, so the tree explains <strong>25.2%</strong> of the variance in the species matrix.</p>
<p>How many leaves does it have?
* 2 leaves</p>
<p><strong>Step 2</strong>: Identify the indicator species.</p>
<p>Which species are significant indicator species for each grouping of sites?</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="multivariate-regression-tree.html#cb113-1"></a><span class="co"># Calculate indicator values (indval) for each species</span></span>
<span id="cb113-2"><a href="multivariate-regression-tree.html#cb113-2"></a>mite.mrt.indval &lt;-<span class="st"> </span><span class="kw">indval</span>(mite.spe.hel, mite.mrt<span class="op">$</span>where)</span>
<span id="cb113-3"><a href="multivariate-regression-tree.html#cb113-3"></a></span>
<span id="cb113-4"><a href="multivariate-regression-tree.html#cb113-4"></a><span class="co"># Extract the significant indicator species (and which node</span></span>
<span id="cb113-5"><a href="multivariate-regression-tree.html#cb113-5"></a><span class="co"># they represent)</span></span>
<span id="cb113-6"><a href="multivariate-regression-tree.html#cb113-6"></a>mite.mrt.indval<span class="op">$</span>maxcls[<span class="kw">which</span>(mite.mrt.indval<span class="op">$</span>pval <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.05</span>)]</span></code></pre></div>
<pre><code>##     PHTH     RARD     SSTR  Protopl     MEGR     MPRO     TVIE     HMIN 
##        2        2        2        2        2        2        1        2 
##    HMIN2     TVEL     ONOV     SUCT     LCIL Oribatl1 Ceratoz1 Galumna1 
##        2        2        2        2        1        2        1        2 
## Stgncrs2 Trhypch1     NCOR     SLAT     FSET Lepidzts Miniglmn     LRUG 
##        2        1        1        2        2        2        2        1 
## Ceratoz3 Trimalc2 
##        1        1</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="multivariate-regression-tree.html#cb115-1"></a><span class="co"># Extract their indicator values</span></span>
<span id="cb115-2"><a href="multivariate-regression-tree.html#cb115-2"></a>mite.mrt.indval<span class="op">$</span>indcls[<span class="kw">which</span>(mite.mrt.indval<span class="op">$</span>pval <span class="op">&lt;=</span><span class="st"> </span><span class="fl">0.05</span>)]</span></code></pre></div>
<pre><code>##      PHTH      RARD      SSTR   Protopl      MEGR      MPRO      TVIE      HMIN 
## 0.5317919 0.5584677 0.2256592 0.2517509 0.5769554 0.1567789 0.3793303 0.6421174 
##     HMIN2      TVEL      ONOV      SUCT      LCIL  Oribatl1  Ceratoz1  Galumna1 
## 0.6193076 0.7412296 0.6312483 0.6087557 0.7152107 0.5978167 0.4744997 0.5974145 
##  Stgncrs2  Trhypch1      NCOR      SLAT      FSET  Lepidzts  Miniglmn      LRUG 
## 0.3897917 0.4545803 0.4539642 0.2249109 0.6361272 0.2108305 0.1880194 0.6683300 
##  Ceratoz3  Trimalc2 
## 0.3962540 0.4358974</code></pre>

</div>
</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-breiman1984classification">
<p>Breiman, Leo, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. <em>Classification and Regression Trees</em>. CRC press.</p>
</div>
<div id="ref-de2002multivariate">
<p>De’ath, Glenn. 2002. “Multivariate Regression Trees: A New Technique for Modeling Species–Environment Relationships.” <em>Ecology</em> 83 (4): 1105–17.</p>
</div>
</div>
<hr>
<center> 
  <div class="footer">
      All the content of the workshop series is under a <a href="https://creativecommons.org/licenses/by-nc/2.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
  </div>
</center>
            </section>

          </div>
        </div>
      </div>
<a href="variation-partitioning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-discriminant-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
